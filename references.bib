
@article{abhishek_detectron2_2021,
	title = {Detectron2 {Object} {Detection} \& {Manipulating} {Images} using {Cartoonization}},
	volume = {10},
	abstract = {In today's world, there is a rapid increase in the autonomous vehicle. There are various levels of autonomous vehicles depending upon the degree of autonomy-for the lower degree of autonomy driver has more power and functionality for managing, on coming to the fully automated vehicle like Tesla are expected to have full control over the functions. These advances cooperate to plan the vehicle's position and its nearness to everything around it. Because of this, there is popularity for these vehicles, since they give a great deal of advantages to individuals utilizing them. We use the Facebook AI Research software system that implements object detection algorithms, Caffe2 deep learning framework for advanced object detection by offering speedy training. We have also manipulated images to derive insights addressing the issues companies face when making the step from research to production. We have implemented detectron2 object detection for faster detection of objects. There is labeling of the object \& we used manipulation of images using cartoonization.},
	language = {en},
	number = {08},
	journal = {International Journal of Engineering Research},
	author = {Abhishek, Allena Venkata Sai and Kotni, Sonali},
	month = aug,
	year = {2021},
}

@misc{ren_grounding_2024,
	title = {Grounding {DINO} 1.5: {Advance} the "{Edge}" of {Open}-{Set} {Object} {Detection}},
	shorttitle = {Grounding {DINO} 1.5},
	url = {http://arxiv.org/abs/2405.10300},
	abstract = {This paper introduces Grounding DINO 1.5, a suite of advanced open-set object detection models developed by IDEA Research, which aims to advance the “Edge”1 of open-set object detection. The suite encompasses two models: Grounding DINO 1.5 Pro, a high-performance model designed for stronger generalization capability across a wide range of scenarios, and Grounding DINO 1.5 Edge, an efficient model optimized for faster speed demanded in many applications requiring edge deployment. The Grounding DINO 1.5 Pro model advances its predecessor by scaling up the model architecture, integrating an enhanced vision backbone, and expanding the training dataset to over 20 million images with grounding annotations, thereby achieving a richer semantic understanding. The Grounding DINO 1.5 Edge model, while designed for efficiency with reduced feature scales, maintains robust detection capabilities by being trained on the same comprehensive dataset. Empirical results demonstrate the effectiveness of Grounding DINO 1.5, with the Grounding DINO 1.5 Pro model attaining a 54.3 AP on the COCO detection benchmark and a 55.7 AP on the LVIS-minival zero-shot transfer benchmark, setting new records for open-set object detection. Furthermore, the Grounding DINO 1.5 Edge model, when optimized with TensorRT, achieves a speed of 75.2 FPS while attaining a zero-shot performance of 36.2 AP on the LVIS-minival benchmark, making it more suitable for edge computing scenarios. Model examples and demos with API will be released at https://github.com/IDEA-Research/Grounding-DINO-1.5-API.},
	language = {en},
	urldate = {2024-10-03},
	publisher = {arXiv},
	author = {Ren, Tianhe and Jiang, Qing and Liu, Shilong and Zeng, Zhaoyang and Liu, Wenlong and Gao, Han and Huang, Hongjie and Ma, Zhengyu and Jiang, Xiaoke and Chen, Yihao and Xiong, Yuda and Zhang, Hao and Li, Feng and Tang, Peijun and Yu, Kent and Zhang, Lei},
	month = may,
	year = {2024},
	note = {arXiv:2405.10300 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{son_teacherstudent_2024,
	title = {Teacher–{Student} {Model} {Using} {Grounding} {DINO} and {You} {Only} {Look} {Once} for {Multi}-{Sensor}-{Based} {Object} {Detection}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/14/6/2232},
	doi = {10.3390/app14062232},
	abstract = {Object detection is a crucial research topic in the fields of computer vision and artificial intelligence, involving the identification and classification of objects within images. Recent advancements in deep learning technologies, such as YOLO (You Only Look Once), Faster-R-CNN, and SSDs (Single Shot Detectors), have demonstrated high performance in object detection. This study utilizes the YOLOv8 model for real-time object detection in environments requiring fast inference speeds, specifically in CCTV and automotive dashcam scenarios. Experiments were conducted using the ‘Multi-Image Identical Situation and Object Identification Data’ provided by AI Hub, consisting of multi-image datasets captured in identical situations using CCTV, dashcams, and smartphones. Object detection experiments were performed on three types of multi-image datasets captured in identical situations. Despite the utility of YOLO, there is a need for performance improvement in the AI Hub dataset. Grounding DINO, a zero-shot object detector with a high mAP performance, is employed. While efficient auto-labeling is possible with Grounding DINO, its processing speed is slower than YOLO, making it unsuitable for real-time object detection scenarios. This study conducts object detection experiments using publicly available labels and utilizes Grounding DINO as a teacher model for auto-labeling. The generated labels are then used to train YOLO as a student model, and performance is compared and analyzed. Experimental results demonstrate that using auto-generated labels for object detection does not lead to degradation in performance. The combination of auto-labeling and manual labeling significantly enhances performance. Additionally, an analysis of datasets containing data from various devices, including CCTV, dashcams, and smartphones, reveals the impact of different device types on the recognition accuracy for distinct devices. Through Grounding DINO, this study proves the efficacy of auto-labeling technology in contributing to efficiency and performance enhancement in the field of object detection, presenting practical applicability.},
	language = {en},
	number = {6},
	urldate = {2024-10-03},
	journal = {Applied Sciences},
	author = {Son, Jinhwan and Jung, Heechul},
	month = jan,
	year = {2024},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {auto-labeling, computer vision, deep learning, object detection},
	pages = {2232},
}

@inproceedings{varghese_yolov8_2024,
	title = {{YOLOv8}: {A} {Novel} {Object} {Detection} {Algorithm} with {Enhanced} {Performance} and {Robustness}},
	shorttitle = {{YOLOv8}},
	url = {https://ieeexplore.ieee.org/document/10533619},
	doi = {10.1109/ADICS58448.2024.10533619},
	abstract = {In recent years, the You Only Look Once (YOLO) series of object detection algorithms have garnered significant attention for their speed and accuracy in real-time applications. This paper presents YOLOv8, a novel object detection algorithm that builds upon the advancements of previous iterations, aiming to further enhance performance and robustness. Inspired by the evolution of YOLO architectures from YOLOv1 to YOLOv7, as well as insights from comparative analyses of models like YOLOv5 and YOLOv6, YOLOv8 incorporates key innovations to achieve optimal speed and accuracy. Leveraging attention mechanisms and dynamic convolution, YOLOv8 introduces improvements specifically tailored for small object detection, addressing challenges highlighted in YOLOv7. Additionally, the integration of voice recognition techniques enhances the algorithm's capabilities for video-based object detection, as demonstrated in YOLOv7. The proposed algorithm undergoes rigorous evaluation against state-of-the-art benchmarks, showcasing superior performance in terms of both detection accuracy and computational efficiency. Experimental results on various datasets confirm the effectiveness of YOLOv8 across diverse scenarios, further validating its suitability for real-world applications. This paper contributes to the ongoing advancements in object detection research by presenting YOLOv8 as a versatile and high-performing algorithm, poised to address the evolving needs of computer vision systems.},
	urldate = {2024-10-02},
	booktitle = {2024 {International} {Conference} on {Advances} in {Data} {Engineering} and {Intelligent} {Computing} {Systems} ({ADICS})},
	author = {Varghese, Rejin and M., Sambath},
	month = apr,
	year = {2024},
	keywords = {Benchmark testing, Computational Efficiency, Computer Vision Systems, Computer vision, Heuristic algorithms, Object Detection, Performance Enhancement, Performance evaluation, Robustness, Speech recognition, Technological innovation, YOLO, YOLOv8},
	pages = {1--6},
}

@misc{liu_grounding_2024,
	title = {Grounding {DINO}: {Marrying} {DINO} with {Grounded} {Pre}-{Training} for {Open}-{Set} {Object} {Detection}},
	shorttitle = {Grounding {DINO}},
	url = {http://arxiv.org/abs/2303.05499},
	doi = {10.48550/arXiv.2303.05499},
	abstract = {In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a \$52.5\$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean \$26.1\$ AP. Code will be available at {\textbackslash}url\{https://github.com/IDEA-Research/GroundingDINO\}.},
	urldate = {2024-10-02},
	publisher = {arXiv},
	author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
	month = jul,
	year = {2024},
	note = {arXiv:2303.05499 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html},
	urldate = {2024-10-02},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	year = {2016},
	pages = {779--788},
}

@article{ristic-durrant_review_2021,
	title = {A {Review} of {Vision}-{Based} {On}-{Board} {Obstacle} {Detection} and {Distance} {Estimation} in {Railways}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/10/3452},
	doi = {10.3390/s21103452},
	abstract = {This paper provides a review of the literature on vision-based on-board obstacle detection and distance estimation in railways. Environment perception is crucial for autonomous detection of obstacles in a vehicle’s surroundings. The use of on-board sensors for road vehicles for this purpose is well established, and advances in Artificial Intelligence and sensing technologies have motivated significant research and development in obstacle detection in the automotive field. However, research and development on obstacle detection in railways has been less extensive. To the best of our knowledge, this is the first comprehensive review of on-board obstacle detection methods for railway applications. This paper reviews currently used sensors, with particular focus on vision sensors due to their dominant use in the field. It then discusses and categorizes the methods based on vision sensors into methods based on traditional Computer Vision and methods based on Artificial Intelligence.},
	language = {en},
	number = {10},
	urldate = {2024-10-02},
	journal = {Sensors},
	author = {Ristić-Durrant, Danijela and Franke, Marten and Michels, Kai},
	month = jan,
	year = {2021},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {AI-based vision, autonomous obstacle detection, on-board vision sensors, railways, traditional computer vision},
	pages = {3452},
}

@inproceedings{hu_novel_2019,
	title = {A {Novel} {Algorithm} for {Efficient} {Labeling} and {Its} {Application} to {On}-{Road} {Risk} {Detection}},
	url = {https://ieeexplore.ieee.org/document/8890380},
	doi = {10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00141},
	abstract = {This paper reports on the application of deep learning for on-road risk detection. In the proposed system, a USB-camera is mounted on a mobility scooter to provide video data in real-time, and a convolutional neural network (CNN) is used to detect possible risks. Each frame is classified to 11 categories, including normal, left/right attention, left/right warning, etc. A bottleneck problem in deep learning is the collection of labeled data. During the initial experiment, we collected video data containing more than 130,000 frames to train the CNN. A great number of data will be needed in the process of commercializing the system. To solve this problem, we propose a novel method that enables us to assign labels efficiently. Using this proposed method, we found experimentally that we can obtain labels of all data by labeling manually less than 10\% of all the data. In addition, the CNN obtained via transfer learning, based on the well-known AlexNet, performs very well. The average performance of several runs is about 95.83\% for testing data. Considering that 30 frames are captured in each second, this accuracy means that three consecutive mistakes are almost impossible, if we use the CNN for real-time risk detection.},
	urldate = {2024-10-02},
	booktitle = {2019 {IEEE} {Intl} {Conf} on {Dependable}, {Autonomic} and {Secure} {Computing}, {Intl} {Conf} on {Pervasive} {Intelligence} and {Computing}, {Intl} {Conf} on {Cloud} and {Big} {Data} {Computing}, {Intl} {Conf} on {Cyber} {Science} and {Technology} {Congress} ({DASC}/{PiCom}/{CBDCom}/{CyberSciTech})},
	author = {Hu, Yixin and Zhao, Qiangfu and Tomioka, Yoichi},
	month = aug,
	year = {2019},
	keywords = {Cameras, Data models, Deep learning, Labeling, Motorcycles, Real-time systems, Semisupervised learning, convolutional neural network, data labeling, deep learning, driver assistance, on-road risk detection, transfer learning},
	pages = {754--760},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2024-10-01},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
}

@article{quach_evaluating_2023,
	title = {Evaluating the {Effectiveness} of {YOLO} {Models} in {Different} {Sized} {Object} {Detection} and {Feature}-{Based} {Classification} of {Small} {Objects}},
	volume = {14},
	doi = {10.12720/jait.14.5.907-917},
	abstract = {The YOLO tool has been increasingly developed to assist in object classification. However, the problem of object classification has many difficulties, including small objects, background effects, or noise loss of information. Therefore, to evaluate the objective of classifying small, information-losing objects, the research team installed and evaluated new YOLO models such as YOLOv5, YOLOv6, and YOLOv7. In addition, the study conducted a feature-based object classification test for comparison and evaluation. The study was conducted on the self-collected data set, divided into 2 datasets: a dataset used to evaluate object classification and a dataset used to classify by features. The evaluation results show certain advantages of the YOLOv7 model on parameters such as Precision, Recall, and a mAP threshold of 50\%. The evaluation results show certain advantages of the YOLOv7 model on parameters such as Precision, Recall, and a mAP threshold of 50\%. The study results show that YOLOv7 achieves specific effects when the accuracy of object recognition is over 90\%, in which the feature-based classification also achieves an accuracy of over 70\%. This issue may need different future studies in object recognition and object feature recognition.},
	journal = {Journal of Advances in Information Technology},
	author = {Quach, Luyl Da and Nguyen, Khang and Nguyen Quynh, Anh and Tran Ngoc, Hoang},
	month = sep,
	year = {2023},
	pages = {907--917},
}

@article{safaldin_improved_2024,
	title = {An {Improved} {YOLOv8} to {Detect} {Moving} {Objects}},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10508365/?arnumber=10508365},
	doi = {10.1109/ACCESS.2024.3393835},
	abstract = {Deep learning has revolutionized object detection, with YOLO (You Only Look Once) leading in real-time accuracy. However, detecting moving objects in visual streams presents distinct challenges. This paper proposes a refined YOLOv8 object detection model, emphasizing motion-specific detections in varied visual contexts. Through tailored preprocessing and architectural adjustments, we heighten the model’s sensitivity to object movements. Rigorous testing against KITTI, LASIESTA, PESMOD, and MOCS benchmark datasets revealed that the modified YOLOv8 outperforms the state-of-the-art detection models, especially in environments with significant movement. Specifically, our model achieved an accuracy of 90\%, a mean Average Precision (mAP) of 90\%, and maintained a processing speed of 30 frames per second (FPS), with an Intersection over Union (IoU) score of 80\%. This paper offers a detailed insight into object trajectories, proving invaluable in areas like security, traffic management, and film analysis where motion understanding is critical. As the importance of dynamic scene interpretation grows in artificial intelligence and computer vision, the proposed enhanced YOLOv8 detection model highlights the potential of specialized object detection and underscores the significance of our findings in the evolving field of object detection.},
	urldate = {2024-09-29},
	journal = {IEEE Access},
	author = {Safaldin, Mukaram and Zaghden, Nizar and Mejdoub, Mahmoud},
	year = {2024},
	note = {Conference Name: IEEE Access},
	keywords = {Computer architecture, Deep learning, Detectors, Feature extraction, Object recognition, Real-time systems, Task analysis, YOLO, localization, object detection, segmentation},
	pages = {59782--59806},
}

@inproceedings{du_fused_2017,
	title = {Fused {DNN}: {A} {Deep} {Neural} {Network} {Fusion} {Approach} to {Fast} and {Robust} {Pedestrian} {Detection}},
	shorttitle = {Fused {DNN}},
	url = {https://ieeexplore.ieee.org/abstract/document/7926694},
	doi = {10.1109/WACV.2017.111},
	abstract = {We propose a deep neural network fusion architecture for fast and robust pedestrian detection. The proposed network fusion architecture allows for parallel processing of multiple networks for speed. A single shot deep convolutional network is trained as a object detector to generate all possible pedestrian candidates of different sizes and occlusions. This network outputs a large variety of pedestrian candidates to cover the majority of ground-truth pedestrians while also introducing a large number of false positives. Next, multiple deep neural networks are used in parallel for further refinement of these pedestrian candidates. We introduce a soft-rejection based network fusion method to fuse the soft metrics from all networks together to generate the final confidence scores. Our method performs better than existing state-of-the-arts, especially when detecting small-size and occluded pedestrians. Furthermore, we propose a method for integrating pixel-wise semantic segmentation network into the network fusion architecture as a reinforcement to the pedestrian detector. The approach outperforms state-of-the-art methods on most protocols on Caltech Pedestrian dataset, with significant boosts on several protocols. It is also faster than all other methods.},
	urldate = {2024-09-29},
	booktitle = {2017 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Du, Xianzhi and El-Khamy, Mostafa and Lee, Jungwon and Davis, Larry},
	month = mar,
	year = {2017},
	keywords = {Computer architecture, Context, Detectors, Fuses, Generators, Neural networks, Semantics},
	pages = {953--961},
}

@article{yan_inclined_2024,
	title = {Inclined {Obstacle} {Recognition} and {Ranging} {Method} in {Farmland} {Based} on {Improved} {YOLOv8}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2032-6653},
	url = {https://www.mdpi.com/2032-6653/15/3/104},
	doi = {10.3390/wevj15030104},
	abstract = {Unmanned tractors under ploughing conditions suffer from body tilting, violent shaking and limited hardware resources, which can reduce the detection accuracy of unmanned tractors for field obstacles. We optimize the YOLOv8 model in three aspects: improving the accuracy of detecting tilted obstacles, computational reduction, and adding a visual ranging mechanism. By introducing Funnel ReLU, a self-constructed inclined obstacle dataset, and embedding an SE attention mechanism, these three methods improve detection accuracy. By using MobileNetv2 and Bi FPN, computational reduction, and adding camera ranging instead of LIDAR ranging, the hardware cost is reduced. After completing the model improvement, comparative tests and real-vehicle validation are carried out, and the validation results show that the average detection accuracy of the improved model reaches 98.84\% of the mAP value, which is 2.34\% higher than that of the original model. The computation amount of the same image is reduced from 2.35 billion floating-point computations to 1.28 billion, which is 45.53\% less than the model computation amount. The monitoring frame rate during the movement of the test vehicle reaches 67 FPS, and the model meets the performance requirements of unmanned tractors under normal operating conditions.},
	language = {en},
	number = {3},
	urldate = {2024-09-29},
	journal = {World Electric Vehicle Journal},
	author = {Yan, Xianghai and Chen, Bingxin and Liu, Mengnan and Zhao, Yifan and Xu, Liyou},
	month = mar,
	year = {2024},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {algorithm improvement, camera ranging, dedicated dataset, inclined working condition, unmanned tractor},
	pages = {104},
}

@article{gao_obstacle_2024,
	title = {Obstacle {Detection} {Technology} for {Autonomous} {Driving} {Based} on {Deep} {Learning}},
	volume = {3},
	doi = {10.62051/c3evm786},
	abstract = {With the rapid growth of artificial intelligence (AI) technology, traditional obstacle detection equipment faces multiple challenges such as high cost, low real-time performance, non normalization, dependence on manual operation, and time-consuming and labor-intensive. To address these shortcomings, this article proposes a deep learning (DL) based obstacle detection technology for autonomous driving on the road surface. As a complex system that integrates multiple key components such as environmental perception, positioning and navigation, path planning, and motion control, one of the core technologies of autonomous vehicles is accurate perception of the surrounding environment. In practical applications, autonomous vehicles often face complex and variable road environments, which may lead to a decrease in the quality of images captured by cameras, resulting in blurry and unclear phenomena. The DL method, especially the object detection algorithm, has shown unique advantages in visual perception and recognition in autonomous driving scenes. This paper deeply studies the obstacle detection technology of automatic driving road based on DL, aiming to achieve efficient and accurate obstacle recognition, improve the safety and reliability of auto drive system, and promote the further growth of automatic driving technology.},
	journal = {Transactions on Computer Science and Intelligent Systems Research},
	author = {Gao, Chenhao},
	month = apr,
	year = {2024},
	pages = {117--122},
}

@article{wenning_anomaly_2022,
	title = {Anomaly detection as vision-based obstacle detection for vehicle automation in industrial environment},
	volume = {2},
	issn = {2813-0359},
	url = {https://www.frontiersin.org/journals/manufacturing-technology/articles/10.3389/fmtec.2022.918343/full},
	doi = {10.3389/fmtec.2022.918343},
	abstract = {{\textless}p{\textgreater}Nowadays, produced cars are equipped with mechatronical actuators as well as with a wide range of sensors in order to realize driver assistance functions. These components could enable cars’ automation at low speeds on company premises, although autonomous driving in public traffic is still facing technical and legal challenges. For automating vehicles in an industrial environment a reliable obstacle detection system is required. State-of-the-art solution for protective devices in Automated Guided Vehicles is the distance measuring laser scanner. Since laser scanners are not basic equipment of today’s cars in contrast to monocameras mounted behind the windscreen, we develop a computer vision algorithm that is able to detect obstacles in camera images reliably. Therefore, we make use of our well-known operational design domain by teaching an anomaly detection how the vehicle path should look like. The result is an anomaly detection algorithm that consists of a pre-trained feature extractor and a shallow classifier, modelling the probability of occurrence. We record a data set of a real industrial environment and show a robust classifier after training the algorithm with images of only one run. The performance as an obstacle detection is on par with a semantic segmentation, but requires a fraction of the training data and no labeling.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-09-29},
	journal = {Frontiers in Manufacturing Technology},
	author = {Wenning, Marius and Adlon, Tobias and Burggräf, Peter},
	month = aug,
	year = {2022},
	note = {Publisher: Frontiers},
	keywords = {Anomaly detection (AD), Automated guided vehicle, Autonomous transport, Obstacle detection, obstacle avoidance},
}

@misc{noguchi_road_2024,
	title = {Road {Obstacle} {Detection} based on {Unknown} {Objectness} {Scores}},
	url = {http://arxiv.org/abs/2403.18207},
	doi = {10.48550/arXiv.2403.18207},
	abstract = {The detection of unknown traffic obstacles is vital to ensure safe autonomous driving. The standard object-detection methods cannot identify unknown objects that are not included under predefined categories. This is because object-detection methods are trained to assign a background label to pixels corresponding to the presence of unknown objects. To address this problem, the pixel-wise anomaly-detection approach has attracted increased research attention. Anomaly-detection techniques, such as uncertainty estimation and perceptual difference from reconstructed images, make it possible to identify pixels of unknown objects as out-of-distribution (OoD) samples. However, when applied to images with many unknowns and complex components, such as driving scenes, these methods often exhibit unstable performance. The purpose of this study is to achieve stable performance for detecting unknown objects by incorporating the object-detection fashions into the pixel-wise anomaly detection methods. To achieve this goal, we adopt a semantic-segmentation network with a sigmoid head that simultaneously provides pixel-wise anomaly scores and objectness scores. Our experimental results show that the objectness scores play an important role in improving the detection performance. Based on these results, we propose a novel anomaly score by integrating these two scores, which we term as unknown objectness score. Quantitative evaluations show that the proposed method outperforms state-of-the-art methods when applied to the publicly available datasets.},
	urldate = {2024-09-29},
	publisher = {arXiv},
	author = {Noguchi, Chihiro and Ohgushi, Toshiaki and Yamanaka, Masao},
	month = mar,
	year = {2024},
	note = {arXiv:2403.18207 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{farheen_object_2022,
	title = {Object {Detection} and {Navigation} {Strategy} for {Obstacle} {Avoidance} {Applied} to {Autonomous} {Wheel} {Chair} {Driving}},
	url = {https://ieeexplore.ieee.org/document/9796979},
	doi = {10.1109/IETC54973.2022.9796979},
	abstract = {The primary aim of this study is to develop machine learning or deep-learning aided procedures that enhances the capability of a commercial non-autonomous wheelchair towards autonomy. The paper addresses the computer vision work for obstacle detection applied to an autonomous wheelchair operation. The computer vision tasks including the depth image classification are accommodated in a small form factored and resource constraint computers such as Raspberry Pie and Google Coral. The tasks and strategies also include classifying the images using a pretrained model (TensorFlow lite), detecting and measure the degree of obstacle avoidance by pairing RGB image classification with depth images. The objective has been further extended to develop a simulation platform for autonomous wheelchair driving where navigation and path mapping construction algorithm evaluations are visually offered using MATLAB®.},
	urldate = {2024-09-29},
	booktitle = {2022 {Intermountain} {Engineering}, {Technology} and {Computing} ({IETC})},
	author = {Farheen, Nusrat and Jaman, Golam Gause and Schoen, Marco P.},
	month = may,
	year = {2022},
	keywords = {Computational modeling, Computer vision, Mobile robots, Navigation, Object detection, Wheelchairs, Wheels, autonomous wheelchair, convolutional neural network, depth image, mobile robot, navigation, object detection., obstacle avoidance},
	pages = {1--5},
}

@inproceedings{andreev_runway_2021,
	title = {Runway {Obstacle} {Detection} for {Flight} {Vision} {Systems}},
	url = {https://ieeexplore.ieee.org/document/9396583},
	doi = {10.1109/ElConRus51938.2021.9396583},
	abstract = {Safety of aircraft operations on approach and landing presents a challenging task of air traffic management. Obstacles presence awareness and the runway condition control is extremely required in challenging weather conditions. The paper proposes the obstacles detection method for modern flight vision systems. The topic is relevant to of avionics, computer vision and image processing.},
	urldate = {2024-09-29},
	booktitle = {2021 {IEEE} {Conference} of {Russian} {Young} {Researchers} in {Electrical} and {Electronic} {Engineering} ({ElConRus})},
	author = {Andreev, Denis S.},
	month = jan,
	year = {2021},
	note = {ISSN: 2376-6565},
	keywords = {Aerospace electronics, Air traffic control, Machine vision, Meteorology, Safety, System performance, Task analysis, background subtraction, flight vision system, objects segmentation, runway},
	pages = {1596--1598},
}

@article{said_obstacle_2023,
	title = {Obstacle {Detection} {System} for {Navigation} {Assistance} of {Visually} {Impaired} {People} {Based} on {Deep} {Learning} {Techniques}},
	volume = {23},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10255983/},
	doi = {10.3390/s23115262},
	abstract = {Visually impaired people seek social integration, yet their mobility is restricted. They need a personal navigation system that can provide privacy and increase their confidence for better life quality. In this paper, based on deep learning and neural architecture search (NAS), we propose an intelligent navigation assistance system for visually impaired people. The deep learning model has achieved significant success through well-designed architecture. Subsequently, NAS has proved to be a promising technique for automatically searching for the optimal architecture and reducing human efforts for architecture design. However, this new technique requires extensive computation, limiting its wide use. Due to its high computation requirement, NAS has been less investigated for computer vision tasks, especially object detection. Therefore, we propose a fast NAS to search for an object detection framework by considering efficiency. The NAS will be used to explore the feature pyramid network and the prediction stage for an anchor-free object detection model. The proposed NAS is based on a tailored reinforcement learning technique. The searched model was evaluated on a combination of the Coco dataset and the Indoor Object Detection and Recognition (IODR) dataset. The resulting model outperformed the original model by 2.6\% in average precision (AP) with acceptable computation complexity. The achieved results proved the efficiency of the proposed NAS for custom object detection.},
	number = {11},
	urldate = {2024-09-29},
	journal = {Sensors (Basel, Switzerland)},
	author = {Said, Yahia and Atri, Mohamed and Albahar, Marwan Ali and Ben Atitallah, Ahmed and Alsariera, Yazan Ahmad},
	month = jun,
	year = {2023},
	pmid = {37299996},
	pmcid = {PMC10255983},
	pages = {5262},
}

@article{fang_computer_2021,
	title = {Computer vision based obstacle detection and target tracking for autonomous vehicles},
	volume = {336},
	doi = {10.1051/matecconf/202133607004},
	abstract = {Obstacle detection and target tracking are two major issues for intelligent autonomous vehicles. This paper proposes a new scheme to achieve target tracking and real-time obstacle detection of obstacles based on computer vision. ResNet-18 deep learning neural network is utilized for obstacle detection and Yolo-v3 deep learning neural network is employed for real-time target tracking. These two trained models can be deployed on an autonomous vehicle equipped with an NVIDIA Jetson Nano motherboard. The autonomous vehicle moves to avoid obstacles and follow tracked targets by camera. Adjusting the steering and movement of the autonomous vehicle according to the PID algorithm during the movement, therefore, will help the proposed vehicle achieve stable and precise tracking.},
	journal = {MATEC Web of Conferences},
	author = {Fang, Ruoyu and Cai, Cheng},
	month = feb,
	year = {2021},
	pages = {07004},
}

@inproceedings{wadhwa_comparison_2023,
	title = {Comparison of {YOLOv8} and {Detectron2} on {Crowd} {Counting} techniques},
	url = {https://ieeexplore.ieee.org/document/10391466/?arnumber=10391466},
	doi = {10.1109/ISAS60782.2023.10391466},
	abstract = {With the increase of security and surveillance cameras in public spaces has also created a demand for analyzing them and Crowd counting has gained a lot of popularity in recent times as it is viewed as a one of the major components for ensuring public safety and monitoring as it is a very laborious task for a person to sit in front of a security footage. It can be a very challenging task to create a dataset for training an algorithm and laborious data annotation is required for training and testing of the algorithms. This research paper discusses the 2 popular approaches of crowd counting in use today and examines their key features. The techniques used here are YOLOvS and Detectron2 which can be used for object and image detection and segmentation. YOLO stands for”You Only Look Once” and it is a popular algorithm as it is suitable for a wide variety of object detection, image segmentation and image classification tasks.Detectron2 is a Facebook AI Research (FAIR)’s library for object detection and segmentation in images. In our research, we found YOLOvS performance is better than detectron2. Detectron2 has been performed better than YOLOvS in terms of overall accuracy, but it has been observed on the basis of the experiments conducted above that YOLOvS is able to detect more objects than Detectron},
	urldate = {2024-09-29},
	booktitle = {2023 7th {International} {Symposium} on {Innovative} {Approaches} in {Smart} {Technologies} ({ISAS})},
	author = {Wadhwa, Mehul and Choudhury, Tanupriya and Raj, Gaurav and Patni, Jagdish Chandra},
	month = nov,
	year = {2023},
	keywords = {Computer Vision, Crowd Counting, Detectron2, Image segmentation, Public security, Real-time systems, Social networking (online), Surveillance, Training, YOLO, YOLOv8},
	pages = {1--6},
}

@article{badrloo_image-based_2022,
	title = {Image-{Based} {Obstacle} {Detection} {Methods} for the {Safe} {Navigation} of {Unmanned} {Vehicles}: {A} {Review}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {Image-{Based} {Obstacle} {Detection} {Methods} for the {Safe} {Navigation} of {Unmanned} {Vehicles}},
	url = {https://www.mdpi.com/2072-4292/14/15/3824},
	doi = {10.3390/rs14153824},
	abstract = {Mobile robots lack a driver or a pilot and, thus, should be able to detect obstacles autonomously. This paper reviews various image-based obstacle detection techniques employed by unmanned vehicles such as Unmanned Surface Vehicles (USVs), Unmanned Aerial Vehicles (UAVs), and Micro Aerial Vehicles (MAVs). More than 110 papers from 23 high-impact computer science journals, which were published over the past 20 years, were reviewed. The techniques were divided into monocular and stereo. The former uses a single camera, while the latter makes use of images taken by two synchronised cameras. Monocular obstacle detection methods are discussed in appearance-based, motion-based, depth-based, and expansion-based categories. Monocular obstacle detection approaches have simple, fast, and straightforward computations. Thus, they are more suited for robots like MAVs and compact UAVs, which usually are small and have limited processing power. On the other hand, stereo-based methods use pair(s) of synchronised cameras to generate a real-time 3D map from the surrounding objects to locate the obstacles. Stereo-based approaches have been classified into Inverse Perspective Mapping (IPM)-based and disparity histogram-based methods. Whether aerial or terrestrial, disparity histogram-based methods suffer from common problems: computational complexity, sensitivity to illumination changes, and the need for accurate camera calibration, especially when implemented on small robots. In addition, until recently, both monocular and stereo methods relied on conventional image processing techniques and, thus, did not meet the requirements of real-time applications. Therefore, deep learning networks have been the centre of focus in recent years to develop fast and reliable obstacle detection solutions. However, we observed that despite significant progress, deep learning techniques also face difficulties in complex and unknown environments where objects of varying types and shapes are present. The review suggests that detecting narrow and small, moving obstacles and fast obstacle detection are the most challenging problem to focus on in future studies.},
	language = {en},
	number = {15},
	urldate = {2024-09-29},
	journal = {Remote Sensing},
	author = {Badrloo, Samira and Varshosaz, Masood and Pirasteh, Saied and Li, Jonathan},
	month = jan,
	year = {2022},
	note = {Number: 15
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {MAVs, UAV, deep learning methods, image-based, obstacle detection},
	pages = {3824},
}

@inproceedings{jenefa_real-time_2023,
	title = {Real-{Time} {Rail} {Safety}: {A} {Deep} {Convolutional} {Neural} {Network} {Approach} for {Obstacle} {Detection} on {Tracks}},
	shorttitle = {Real-{Time} {Rail} {Safety}},
	url = {https://ieeexplore.ieee.org/document/10125284/?arnumber=10125284},
	doi = {10.1109/ICSPC57692.2023.10125284},
	abstract = {Identifying obstacles on train tracks in real-time can be challenging due to various factors such as visibility, environmental conditions, and the speed of the train. Accurate and efficient detection of obstacles is essential for ensuring the safety of passengers and avoiding derailments. The objective of this study was to develop a deep convolutional neural network (DCNN) to identify obstacles in train tracks. In recent years, the use of DCNNs has been widespread for image recognition and classification tasks, however, there has been limited research in the field of obstacle identification in train tracks. The DCNN was trained on a dataset of train track images with and without obstacles. The dataset consisted of over 1000 images with various types of obstacles, including rocks, trees, and other debris. The DCNN was implemented using the TensorFlow and Keras libraries. The DCNN was able to accurately identify obstacles in train tracks with an overall accuracy of 98\%. The model also showed high sensitivity and specificity in detecting obstacles in train tracks. This study demonstrates the effectiveness of using DCNNs for obstacle identification in train tracks. The high accuracy of the DCNN shows its potential for practical application in real-world scenarios to improve railway safety. Further research is required to test the performance of the DCNN in different environmental conditionsand to optimize its architecture for better performance.},
	urldate = {2024-09-29},
	booktitle = {2023 4th {International} {Conference} on {Signal} {Processing} and {Communication} ({ICSPC})},
	author = {Jenefa, A and Ande, Aaron and Mounikuttan, Thejas and Anuj, M.D. and Jenulin Makros, G and Rejoice, G Rachel and Shalini, T Mary},
	month = mar,
	year = {2023},
	keywords = {Deep Convolutional Neural Network (DCNN), Object Detection, Rail Safety, Rails, Railway Track, Real-time systems, Rocks, Sensitivity and specificity, Signal processing, Support vector machines, Training data},
	pages = {101--105},
}

@article{hindarto_enhancing_2023,
	title = {Enhancing {Road} {Safety} with {Convolutional} {Neural} {Network} {Traffic} {Sign} {Classification}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by-nc/4.0},
	issn = {2541-2019, 2541-044X},
	url = {https://jurnal.polgan.ac.id/index.php/sinkron/article/view/13124},
	doi = {10.33395/sinkron.v8i4.13124},
	abstract = {Recent computer vision and deep learning breakthroughs have improved road safety by automatically classifying traffic signs. This research uses CNNs to classify traffic signs to improve road safety. Autonomous vehicles and intelligent driver assistance systems require accurate traffic sign detection and classification. Using deep learning, we created a CNN model that can recognize and classify road traffic signs. This research uses a massive dataset of labeled traffic sign photos for training and validation. These CNN algorithms evaluate images and produce realtime predictions to assist drivers and driverless cars in understanding traffic signs. Advanced driver assistance systems, navigation systems, and driverless vehicles can use this technology to give drivers more precise information, improving their decision-making and road safety. Researcher optimized CNN model design, training, and evaluation metrics during development. The model was rigorously tested and validated for robustness and classification accuracy. The research also solves realworld driving obstacles like illumination, weather, and traffic signal obstructions. This research shows deep learning-based traffic sign classification can dramatically improve road safety. This technology can prevent accidents and enhance traffic management by accurately recognizing and interpreting traffic signs. It is also a potential step toward a safer, more efficient transportation system with several automotive and intelligent transportation applications. Road safety is a global issue, and CNN-based traffic sign classification can reduce accidents and improve driving. On filter 3, Convolutional Neural Network training accuracy reached 98.9\%, while validation accuracy reached 88.23\%.},
	language = {en},
	number = {4},
	urldate = {2024-09-29},
	journal = {sinkron},
	author = {Hindarto, Djarot},
	month = nov,
	year = {2023},
	pages = {2810--2818},
}

@misc{pedoeem_yolo-lite_2018,
	title = {{YOLO}-{LITE}: {A} {Real}-{Time} {Object} {Detection} {Algorithm} {Optimized} for {Non}-{GPU} {Computers}},
	shorttitle = {{YOLO}-{LITE}},
	url = {http://arxiv.org/abs/1811.05588},
	abstract = {This paper focuses on YOLO-LITE, a real-time object detection model developed to run on portable devices such as a laptop or cellphone lacking a Graphics Processing Unit (GPU). The model was ﬁrst trained on the PASCAL VOC dataset then on the COCO dataset, achieving a mAP of 33.81\% and 12.26\% respectively. YOLO-LITE runs at about 21 FPS on a non-GPU computer and 10 FPS after implemented onto a website with only 7 layers and 482 million FLOPS. This speed is 3.8× faster than the fastest state of art model, SSD MobilenetvI. Based on the original object detection algorithm YOLOV2, YOLOLITE was designed to create a smaller, faster, and more efﬁcient model increasing the accessibility of real-time object detection to a variety of devices.},
	language = {en},
	urldate = {2024-09-29},
	publisher = {arXiv},
	author = {Pedoeem, Jonathan and Huang, Rachel},
	month = nov,
	year = {2018},
	note = {arXiv:1811.05588 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{turay_toward_2022,
	title = {Toward {Performing} {Image} {Classification} and {Object} {Detection} {With} {Convolutional} {Neural} {Networks} in {Autonomous} {Driving} {Systems}: {A} {Survey}},
	volume = {10},
	issn = {2169-3536},
	shorttitle = {Toward {Performing} {Image} {Classification} and {Object} {Detection} {With} {Convolutional} {Neural} {Networks} in {Autonomous} {Driving} {Systems}},
	url = {https://ieeexplore.ieee.org/document/9696317/?arnumber=9696317},
	doi = {10.1109/ACCESS.2022.3147495},
	abstract = {Nowadays Convolutional Neural Networks (CNNs) are being employed in a wide range of industrial technologies for a variety of sectors, such as medical, automotive, aviation, agriculture, space, etc. This paper reviews the state-of-the-art in both the field of CNNs for image classification and object detection and Autonomous Driving Systems (ADSs) in a synergetic way. Layer-based details of CNNs along with parameter and floating-point operation number calculations are outlined. Using an evolutionary approach, the majority of the outstanding image classification CNNs, published in the open literature, is introduced with a focus on their accuracy performance, parameter number, model size, and inference speed, highlighting the progressive developments in convolutional operations. Results of a novel investigation of the convolution types and operations commonly used in CNNs are presented, including a timing analysis aimed at assessing their impact on CNN performance. This extensive experimental study provides new insight into the behaviour of each convolution type in terms of training time, inference time, and layer level decomposition. Building blocks for CNN-based object detection are also discussed, such as backbone networks and baseline types, and then representative state-of-the-art designs are outlined. Experimental results from the literature are summarised for each of the reviewed models. This is followed by an overview of recent ADSs related works and current industry activities, aiming to bridge academic research and industry practice on CNNs and ADSs. Design approaches targeted at solving problems of automakers in achieving real-time implementations are also proposed based on a discussion of design constraints, human vs. machine evaluations and trade-off analysis of accuracy vs. size. Current technologies, promising directions, and expectations from the literature on ADSs are introduced including a comprehensive trade-off analysis from a human-machine perspective.},
	urldate = {2024-09-29},
	journal = {IEEE Access},
	author = {Turay, Tolga and Vladimirova, Tanya},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {Autonomous vehicles, Convolution, Convolutional neural networks, Image classification, Laser radar, Object detection, Real-time systems, Task analysis, computer vision, convolution, convolutional neural networks, embedded systems, image classification, object detection},
	pages = {14076--14119},
}

@article{dairi_obstacle_2018,
	title = {Obstacle {Detection} for {Intelligent} {Transportation} {Systems} {Using} {Deep} {Stacked} {Autoencoder} and k -{Nearest} {Neighbor} {Scheme}},
	volume = {18},
	issn = {1558-1748},
	url = {https://ieeexplore.ieee.org/document/8352801/?arnumber=8352801},
	doi = {10.1109/JSEN.2018.2831082},
	abstract = {Obstacle detection is an essential element for the development of intelligent transportation systems so that accidents can be avoided. In this paper, we propose a stereovision-based method for detecting obstacles in urban environment. The proposed method uses a deep stacked auto-encoders (DSA) model that combines the greedy learning features with the dimensionality reduction capacity and employs an unsupervised k -nearest neighbors (KNN) algorithm to accurately and reliably detect the presence of obstacles. We consider obstacle detection as an anomaly detection problem. We evaluated the proposed method by using practical data from three publicly available data sets, the Malaga stereovision urban data set, the Daimler urban segmentation data set, and the Bahnhof data set. Also, we compared the efficiency of DSA-KNN approach to the deep belief network-based clustering schemes. Results show that the DSA-KNN is suitable to visually monitor urban scenes.},
	number = {12},
	urldate = {2024-09-29},
	journal = {IEEE Sensors Journal},
	author = {Dairi, Abdelkader and Harrou, Fouzi and Sun, Ying and Senouci, Mohamed},
	month = jun,
	year = {2018},
	note = {Conference Name: IEEE Sensors Journal},
	keywords = {Clustering algorithms, Kernel, Machine learning, Machine learning algorithms, Obstacle detection, Partitioning algorithms, Roads, Sensors, autonomous vehicles, clustering algorithms, deep learning, intelligent transportation systems},
	pages = {5122--5132},
}

@article{ling_optimization_2024,
	title = {Optimization of autonomous driving image detection based on {RFAConv} and triplet attention},
	volume = {67},
	issn = {2755-2721, 2755-273X},
	url = {https://www.ewadirect.com/proceedings/ace/article/view/13908},
	doi = {10.54254/2755-2721/67/2024MA0067},
	abstract = {YOLOv8 plays a crucial role in the realm of autonomous driving, owing to its high-speed target detection, precise identification and positioning, and versatile compatibility across multiple platforms. By processing video streams or images in real-time, YOLOv8 rapidly and accurately identifies obstacles such as vehicles and pedestrians on roadways, offering essential visual data for autonomous driving systems. Moreover, YOLOv8 supports various tasks including instance segmentation, image classification, and attitude estimation, thereby providing comprehensive visual perception for autonomous driving, ultimately enhancing driving safety and efficiency. Recognizing the significance of object detection in autonomous driving scenarios and the challenges faced by existing methods, this paper proposes a holistic approach to enhance the YOLOv8 model. The study introduces two pivotal modifications: the C2f\_RFAConv module and the Triplet Attention mechanism. Firstly, the proposed modifications are elaborated upon in the methodological section. The C2f\_RFAConv module replaces the original module to enhance feature extraction efficiency, while the Triplet Attention mechanism enhances feature focus. Subsequently, the experimental procedure delineates the training and evaluation process, encompassing training the original YOLOv8, integrating modified modules, and assessing performance improvements using metrics and PR curves. The results demonstrate the efficacy of the modifications, with the improved YOLOv8 model exhibiting significant performance enhancements, including increased MAP values and improvements in PR curves. Lastly, the analysis section elucidates the results and attributes the performance improvements to the introduced modules. C2f\_RFAConv enhances feature extraction efficiency, while Triplet Attention improves feature focus for enhanced target detection.},
	language = {en},
	number = {1},
	urldate = {2024-09-29},
	journal = {Applied and Computational Engineering},
	author = {Ling, Zhipeng and Xin, Qi and Lin, Yiyu and Su, Guangze and Shui, Zuwei},
	month = jul,
	year = {2024},
	pages = {68--75},
}

@inproceedings{sarda_object_2021,
	title = {Object {Detection} for {Autonomous} {Driving} using {YOLO} [{You} {Only} {Look} {Once}] algorithm},
	url = {https://ieeexplore.ieee.org/document/9388577/?arnumber=9388577},
	doi = {10.1109/ICICV50876.2021.9388577},
	abstract = {The field of autonomous driving is going to be the face of the automobile industry very soon. The number of accidents that take place because of human error currently is very high and it can be slashed to a huge extent with the advent of autonomous driving. One of the primary prerequisites and a huge part of autonomous driving is dependent on object detection through computer vision, this paper aims at aiding towards the field of autonomous driving by helping detect objects with the use of deep learning algorithms. Research work used state-of-the-art algorithm YOLO (you only look once) to detect different objects that appear on the road and classified into the category that they belong to with the help of bounding boxes. The weights of the YOLO v4 is utilized to custom train our model to detect the objects and the data will be collected from the open images dataset using its OIDv4 toolkit.},
	urldate = {2024-09-29},
	booktitle = {2021 {Third} {International} {Conference} on {Intelligent} {Communication} {Technologies} and {Virtual} {Mobile} {Networks} ({ICICV})},
	author = {Sarda, Abhishek and Dixit, Shubhra and Bhan, Anupama},
	month = feb,
	year = {2021},
	keywords = {Autonomous vehicles, Classification algorithms, Deep learning, Faces, Industries, Object detection, Roads, YOLO, autonomous vehicles, computer vision, object detection},
	pages = {1370--1374},
}

@article{wang_efficient_2018,
	title = {Efficient {Rail} {Area} {Detection} {Using} {Convolutional} {Neural} {Network}},
	volume = {6},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8550680/?arnumber=8550680},
	doi = {10.1109/ACCESS.2018.2883704},
	abstract = {Rail area detection is essential in active obstacle perception system of the train. This paper presents an efficient rail area detection method based on the convolutional neural network (CNN). The proposed method is divided into two main parts: extraction of the rail area and further optimization. First, a CNN architecture is established to achieve accurate rail area detection, enabling the pixel-level classification of the rail area. It is notable that the main improvement of our architecture is dilated cascade connection and cascade sampling. Second, an improved polygon fitting method is applied to optimize the contour of the extracted rail area and, thus, obtains a more elegant outline of the rail region. As shown by the experimental results, the excellent accuracy is obtained by using our method, i.e., 98.46\% mean intersection-over-union and 99.15\% mean pixel accuracy on the BH-rail-dataset, and verified the applicability of our detection method in a large-scale traffic scene video frames of Beijing metro Yanfang line and Shanghai metro line 6.},
	urldate = {2024-09-29},
	journal = {IEEE Access},
	author = {Wang, Zhangyu and Wu, Xinkai and Yu, Guizhen and Li, Mingxing},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {Cameras, Convolution, Convolutional neural networks, Feature extraction, Network architecture, Rail area detection, Rail transportation, Rails, convolutional neural network, polygon fitting},
	pages = {77656--77664},
}

@article{zhao_object_2019,
	title = {Object {Detection} {With} {Deep} {Learning}: {A} {Review}},
	volume = {30},
	issn = {2162-2388},
	shorttitle = {Object {Detection} {With} {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/document/8627998/?arnumber=8627998},
	doi = {10.1109/TNNLS.2018.2876865},
	abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.},
	number = {11},
	urldate = {2024-09-29},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
	month = nov,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Computer architecture, Deep learning, Feature extraction, Neural networks, Object detection, Task analysis, Training, neural network, object detection},
	pages = {3212--3232},
}
