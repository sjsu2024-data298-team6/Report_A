
@misc{rao_yolov11_2024,
	title = {{YOLOv11} {Explained}: {Next}-{Level} {Object} {Detection} with {Enhanced} {Speed} and {Accuracy}},
	shorttitle = {{YOLOv11} {Explained}},
	url = {https://medium.com/@nikhil-rao-20/yolov11-explained-next-level-object-detection-with-enhanced-speed-and-accuracy-2dbe2d376f71},
	abstract = {A brief article all about the recently released YOLOv11 from its architecture to its performance. This is all you want to know about.},
	language = {en},
	urldate = {2024-12-09},
	journal = {Medium},
	author = {Rao, S. Nikhileswara},
	month = oct,
	year = {2024},
}

@misc{zhao_detrs_2024,
	title = {{DETRs} {Beat} {YOLOs} on {Real}-time {Object} {Detection}},
	url = {http://arxiv.org/abs/2304.08069},
	doi = {10.48550/arXiv.2304.08069},
	abstract = {The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1\% / 54.3\% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2\% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3\% / 56.2\% AP. The project page: https://zhao-yian.github.io/RTDETR.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Zhao, Yian and Lv, Wenyu and Xu, Shangliang and Wei, Jinman and Wang, Guanzhong and Dang, Qingqing and Liu, Yi and Chen, Jie},
	month = apr,
	year = {2024},
	note = {arXiv:2304.08069 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{krishnan_multiple_2024,
	title = {Multiple {Object} {Detection} and {Re}-{Identification} {Using} {Detectron2}},
	volume = {13},
	doi = {10.15680/IJIRSET.2024.1305321},
	number = {5},
	journal = {International Journal of Innovative Research in Science, Engineering and Technology},
	author = {Krishnan, Raji and Athira, T P and Murali, Mahesh},
	month = may,
	year = {2024},
	pages = {8524--8529},
}

@inproceedings{byzkrovnyi_comparison_2023,
	address = {Kharkiv, Ukraine},
	title = {Comparison of {Object} {Detection} {Algorithms} for the {Task} of {Detecting} {Possible} {Road} {Accident}},
	volume = {Computational Linguistics and Intelligent Systems 2023},
	abstract = {This article is an investigation of the best suited algorithm for object detection in terms of finding a potential road accident case. The determination of the best algorithm is based on comparative analysis of evaluation and testing results and metrics. Also, the max FPS of processing video during detection will be considered. There are a lot of examples of road accident situations, but in this study the turn left across one and more oncoming road lanes will be explained. There are two classes for recognition: danger and not-danger which explain case. Other accident types have not been considered due to wish for making simple models with further sophistication. The Yolov7 and Detectron2 algorithms are compared.},
	language = {en},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Computational} {Linguistics} and {Intelligent} {Systems}. {Volume} {I}: {Machine} {Learning} {Workshop}},
	author = {Byzkrovnyi, Oleksandr and Chupryna, Anastasiya and Smelyakov, Kirill and Sharonova, Natalia and Repikhov, Vadim},
	month = apr,
	year = {2023},
	pages = {13--28},
}

@inproceedings{vazquez_landing_2024,
	address = {Bristol, United Kingdom},
	title = {Landing {Zone} {Detection} for {MAVs} using {Depth} {Images} and {Vision} {Transformers}},
	abstract = {We present a methodology for landing zone detection in Micro Aerial Vehicles (MAVs) using Vision Transformers (ViTs). We present results on the use of ViTs due to their ability to capture spatial relations through the attention mechanism, potentially offering superior performance with fewer training examples than other Deep Neural Networks. Experiments with aerial images, from a dataset and depth images captured with a depth camera on board a drone, confirm ViT’s superiority over other widely used Convolutional Network such as ResNet, particularly with limited training data. Despite the noisy depth images, captured with the depth camera, the ViT model can be used to detect landing zones with an average processing time of 11.9 ms on outdated GPU hardware.},
	language = {en},
	booktitle = {15th annual {International} {Micro} {Air} {Vehicle} {Conference} and {Competition}},
	author = {Vazquez, Victoria and Martinez-Carranza, Jose},
	editor = {Richardson, T.},
	month = sep,
	year = {2024},
	note = {Paper no. IMAV2024-19},
	pages = {163--170},
}

@article{ye_real-time_2023,
	title = {Real-{Time} {Object} {Detection} {Network} in {UAV}-{Vision} {Based} on {CNN} and {Transformer}},
	volume = {72},
	issn = {1557-9662},
	url = {https://ieeexplore.ieee.org/document/10035490},
	doi = {10.1109/TIM.2023.3241825},
	abstract = {Unmanned aerial vehicles (UAVs) play an important role in conducting automatic patrol inspections of cities, which can ensure the safety of urban residents’ life and property and the normal operation of cities. However, during the inspection process, problems may arise. For example, numerous small objects in UAV images are difficult to detect, objects in UAV images are severely occluded, and requirements for real-time performances are posed. To address these issues, we first propose a real-time object detection network (RTD-Net) for UAV images. Besides, to deal with the lack of visual features of small objects, we design a feature fusion module (FFM) to interact and fuse features at different levels and improve the feature expression ability of small objects. To achieve real-time detection, we design a lightweight feature extraction module (LEM) to build the backbone network to control the calculation quantity and parameters. To solve the issue of discontinuous features of occluded objects, an efficient convolutional transformer block (ECTB)-based convolutional multihead self-attention (CMHSA) is designed to improve the recognition ability of occluded objects by extracting the context information of objects. Compared with multihead self-attention (MHSA) in the traditional transformer, CMHSA uses convolutional projection to replace the position-linear projection, which can reduce a large amount of calculation without performance loss. Finally, an attention prediction head (APH) is designed based on the attention mechanism to improve the ability of the model to extract attention regions in complex scenarios. The proposed method reaches a detection accuracy of 86.4\% mean average precision (mAP) in our UAV image dataset. In addition, it achieves a detection accuracy of 86.0\% mAP and a detection speed of 33.4 frames/s in the NVIDIA Jeston TX2 embedded device.},
	urldate = {2024-11-10},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Ye, Tao and Qin, Wenyang and Zhao, Zongyang and Gao, Xiaozhi and Deng, Xiangpeng and Ouyang, Yu},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Instrumentation and Measurement},
	keywords = {Automatic patrol inspections, Convolutional neural networks, Feature extraction, Inspection, Object detection, Real-time systems, Support vector machines, Transformers, multiscale object detection, real-time detection, transformer, unmanned aerial vehicle (UAV)-vision},
	pages = {1--13},
}

@inproceedings{ding_improved_2022,
	title = {Improved object detection algorithm for drone-captured dataset based on yolov5},
	url = {https://ieeexplore.ieee.org/document/9712813},
	doi = {10.1109/ICCECE54139.2022.9712813},
	abstract = {Object detection is a basic task on computer vision, recently drone-captured scenarios had a wide range of applications in the industry. In this paper, firstly we will introduce some characteristics of yolov5s structure and some defects of yolov5s on tiny-size object detection. Secondly, to solve the defects mentioned above we will focus on the improvement based on the yolov5s structure to VisDrone dataset. Due to some characteristics of UAV dataset, the target scale changes greatly at different flight altitude, some target areas captured by drone show high density, which may bring some blurring or occlusion and some images may cover a large area, so we try to improve the structure of yolov5 for better performance. An additional prediction head is added to detect tiny-scale targets, which is also used to deal with large size variance of objects, and an EPSA net module is added in the middle of the backbone to explore the self-attention potential of feature representation. Moreover, extra up sampling is added in the neck part to optimize the detection of tiny-size target samples. Finally, it is proved by experiments that new yolov5s structure improve the value of mAP@.5 about 7\% and improve the value of mAP@.5:.95 about 5\% on VisDrone-2019-DET dataset.},
	urldate = {2024-11-10},
	booktitle = {2022 2nd {International} {Conference} on {Consumer} {Electronics} and {Computer} {Engineering} ({ICCECE})},
	author = {Ding, Kaiwen and Li, Xianjiang and Guo, Weijie and Wu, Liaoni},
	month = jan,
	year = {2022},
	keywords = {Detectors, Feature extraction, Head, Industries, Neck, Object detection, Transformers, VisDrone, object detection, self-attention, yolov5},
	pages = {895--899},
}

@misc{khoramdel_yolo-former_2024,
	title = {{YOLO}-{Former}: {YOLO} {Shakes} {Hand} {With} {ViT}},
	shorttitle = {{YOLO}-{Former}},
	url = {http://arxiv.org/abs/2401.06244},
	doi = {10.48550/arXiv.2401.06244},
	abstract = {The proposed YOLO-Former method seamlessly integrates the ideas of transformer and YOLOv4 to create a highly accurate and efficient object detection system. The method leverages the fast inference speed of YOLOv4 and incorporates the advantages of the transformer architecture through the integration of convolutional attention and transformer modules. The results demonstrate the effectiveness of the proposed approach, with a mean average precision (mAP) of 85.76{\textbackslash}\% on the Pascal VOC dataset, while maintaining high prediction speed with a frame rate of 10.85 frames per second. The contribution of this work lies in the demonstration of how the innovative combination of these two state-of-the-art techniques can lead to further improvements in the field of object detection.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Khoramdel, Javad and Moori, Ahmad and Borhani, Yasamin and Ghanbarzadeh, Armin and Najafi, Esmaeil},
	month = jan,
	year = {2024},
	note = {arXiv:2401.06244 
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bhattacharya_vision_2024,
	title = {Vision {Transformers} for {End}-to-{End} {Vision}-{Based} {Quadrotor} {Obstacle} {Avoidance}},
	url = {http://arxiv.org/abs/2405.10391},
	doi = {10.48550/arXiv.2405.10391},
	abstract = {We demonstrate the capabilities of an attention-based end-to-end approach for high-speed vision-based quadrotor obstacle avoidance in dense, cluttered environments, with comparison to various state-of-the-art learning architectures. Quadrotor unmanned aerial vehicles (UAVs) have tremendous maneuverability when flown fast; however, as flight speed increases, traditional model-based approaches to navigation via independent perception, mapping, planning, and control modules breaks down due to increased sensor noise, compounding errors, and increased processing latency. Thus, learning-based, end-to-end vision-to-control networks have shown to have great potential for online control of these fast robots through cluttered environments. We train and compare convolutional, U-Net, and recurrent architectures against vision transformer (ViT) models for depth image-to-control in high-fidelity simulation, observing that ViT models are more effective than others as quadrotor speeds increase and in generalization to unseen environments, while the addition of recurrence further improves performance while reducing quadrotor energy cost across all tested flight speeds. We assess performance at speeds of up to 7m/s in simulation and hardware. To the best of our knowledge, this is the first work to utilize vision transformers for end-to-end vision-based quadrotor control.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Bhattacharya, Anish and Rao, Nishanth and Parikh, Dhruv and Kunapuli, Pratik and Wu, Yuwei and Tao, Yuezhan and Matni, Nikolai and Kumar, Vijay},
	month = sep,
	year = {2024},
	note = {arXiv:2405.10391},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{pathak_autonomous_2023,
	title = {Autonomous {Obstacle} {Detection} and {Avoidance} in {Drones}},
	volume = {9},
	doi = {10.36676/irt.2023-v9i4-011},
	abstract = {Drone technology has recently advanced, and object detection technology is already evolving. These technologies can be used to find illegal immigrants, locate missing people and items, and detect industrial and natural disasters. In this research, we investigate how to improve object detection performance in such cases. Photography was carried out in a setting where it was difficult to identify objects. The experimental data was based on images taken under various situations, such as changing the drone's altitude, shooting pictures in the dark, and so on. In this work, we recommend a way to make YOLO models more effective at detecting objects. To determine the key indicators, we will input the collected data into the CNN model and the YOLO model, respectively. Precision, recall, F-1 score, and mAP are the major metrics of evaluation. On the basis of the data comparing the CNN model with the YOLO model, an inference will then be drawn.},
	journal = {Innovative Research Thoughts},
	author = {Pathak, Rachit and Mudunuri, Ajay},
	month = sep,
	year = {2023},
	pages = {73--85},
}

@inproceedings{gao_uav_2024,
	title = {{UAV} {Object} {Detection} {Based} on {Joint} {YOLO} and {Transformer}},
	url = {https://ieeexplore.ieee.org/abstract/document/10695939},
	doi = {10.1109/Ucom62433.2024.10695939},
	abstract = {With the gradual expansion of computer vision application fields, the demand for object detection based on unmanned aerial vehicle (UAV) aerial images continues to grow. Traditional methods have limitations in handling scale changes, motion blur, and complex backgrounds. We propose a novel approach that combines the model You Only Look Once version 5 based on convolutional neural network with the sequence modeling technology Transformer to better capture long-range dependencies and contextual information, thereby improving detection performance. Experimental results on the VisDrone dataset show that the proposed method has comparable performance to existing methods, demonstrating its effectiveness in UAV object detection.},
	urldate = {2024-11-10},
	booktitle = {2024 {International} {Conference} on {Ubiquitous} {Communication} ({Ucom})},
	author = {Gao, Yifan and Ding, Rui and Zhou, Fuhui and Wu, Qihui},
	month = jul,
	year = {2024},
	keywords = {Autonomous aerial vehicles, Computational modeling, Computer architecture, Computer vision, Context modeling, Convolutional neural networks, Head, Neck, Object detection, Transformer, Transformers, UAV, YOLO, You Only Look Once},
	pages = {202--206},
}

@inproceedings{vaidwan_study_2021,
	title = {A study on transformer-based {Object} {Detection}},
	url = {https://ieeexplore.ieee.org/abstract/document/9498550},
	doi = {10.1109/CONIT51480.2021.9498550},
	abstract = {This paper focuses on transformers based end-to-end object detection methods. End to end object detection is a new paradigm that has got attention in recent times. It does not require complex hand-engineered components such as non-max suppression to detect objects inside an image. Various methods are proposed to date to enhance fully end-to-end object detectors, most of them are based on the attention mechanism. In this work, we analyze some algorithms which involve transformers for the purpose of object detection. We discuss end-to-end models in which we have focused on Adaptive clustering-based transformers which solve attention encoder redundancy, Deformable Detection Transformers in which the attention module attends a limited collection of key sampling points, Unsupervisedly pre-trained Detection Transformers which are pre-trained on random query patches from the given image to improve accuracy and finally the Transformer-based Set Prediction using FCOS. These enhanced models not only improve the mean average precision of the model but also improves the total convergence time.},
	urldate = {2024-11-10},
	booktitle = {2021 {International} {Conference} on {Intelligent} {Technologies} ({CONIT})},
	author = {Vaidwan, Hritik and Seth, Nikhil and Parihar, Anil Singh and Singh, Kavinder},
	month = jun,
	year = {2021},
	keywords = {ACT, Adaptation models, Attention, Computational modeling, DETR, Deformable models, Detectors, End to end object detection, Object detection, Redundancy, TSP-FCOS, Training, Transformer, UP-DETR},
	pages = {1--6},
}

@inproceedings{zhang_towards_2023,
	title = {Towards a {High}-{Performance} {Object} {Detector}: {Insights} from {Drone} {Detection} {Using} {ViT} and {CNN}-based {Deep} {Learning} {Models}},
	shorttitle = {Towards a {High}-{Performance} {Object} {Detector}},
	url = {https://ieeexplore.ieee.org/document/10263514?denied=},
	doi = {10.1109/ICSECE58870.2023.10263514},
	abstract = {Accurate drone detection is strongly desired in drone collision avoidance, drone defense and autonomous Unmanned Aerial Vehicle (UAV) self-landing. With the recent emergence of the Vision Transformer (ViT), this critical task is reassessed in this paper using a UAV dataset composed of 1359 drone photos. We construct various CNN and ViT-based models, demonstrating that for single-drone detection, a basic ViT can achieve performance 4.6 times more robust than our best CNN-based transfer learning models. By implementing the state-of-the-art You Only Look Once (YOLO v7, 200 epochs) and the experimental ViT-based You Only Look At One Sequence (YOLOS, 20 epochs) in multi-drone detection, we attain impressive 98\% and 96\% mAP values, respectively. We find that ViT outperforms CNN at the same epoch, but also requires more training data, computational power, and sophisticated, performance-oriented designs to fully surpass the capabilities of cutting-edge CNN detectors. We summarize the distinct characteristics of ViT and CNN models to aid future researchers in developing more efficient deep learning models.},
	urldate = {2024-11-10},
	booktitle = {2023 {IEEE} {International} {Conference} on {Sensors}, {Electronics} and {Computer} {Engineering} ({ICSECE})},
	author = {Zhang, Junyang},
	month = aug,
	year = {2023},
	keywords = {Autonomous aerial vehicles, Computational modeling, Convolutional Neural Network, Deep learning, Detectors, Drone Detection, Sensor phenomena and characterization, Training data, Transfer Learning, Transfer learning, Vision Transformer, You Only Look At One Sequence, You Only Look Once},
	pages = {141--147},
}

@misc{carion_end--end_2020,
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	url = {http://arxiv.org/abs/2005.12872},
	doi = {10.48550/arXiv.2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	month = may,
	year = {2020},
	note = {arXiv:2005.12872},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zhang_vit-yolotransformer-based_2021,
	address = {Montreal, BC, Canada},
	title = {{ViT}-{YOLO}:{Transformer}-{Based} {YOLO} for {Object} {Detection}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66540-191-3},
	shorttitle = {{ViT}-{YOLO}},
	url = {https://ieeexplore.ieee.org/document/9607536/},
	doi = {10.1109/ICCVW54120.2021.00314},
	abstract = {Drone captured images have overwhelming characteristics including dramatic scale variance, complicated background ﬁlled with distractors, and ﬂexible viewpoints, which pose enormous challenges for general object detectors based on common convolutional networks. Recently, the design of vision backbone architectures that use selfattention is an exciting topic. In this work, an improved backbone MHSA-Darknet is designed to retain sufﬁcient global context information and extract more differentiated features for object detection via multi-head self-attention. Regarding the path-aggregation neck, we present a simple yet highly effective weighted bi-directional feature pyramid network (BiFPN) for effectively cross-scale feature fusion. In addition, other techniques including time-test augmentation (TTA) and wighted boxes fusion (WBF) help to achieve better accuracy and robustness. Our experiments demonstrate that ViT-YOLO signiﬁcantly outperforms the state-of-the-art detectors and achieve one of the top results in VisDrone-DET 2021 challenge (39.41 mAP for testchallenge data set and 41 mAP for the test-dev data set).},
	language = {en},
	urldate = {2024-11-10},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCVW})},
	publisher = {IEEE},
	author = {Zhang, Zixiao and Lu, Xiaoqiang and Cao, Guojin and Yang, Yuting and Jiao, Licheng and Liu, Fang},
	month = oct,
	year = {2021},
	pages = {2799--2808},
}

@misc{khanam_yolov11_2024,
	title = {{YOLOv11}: {An} {Overview} of the {Key} {Architectural} {Enhancements}},
	shorttitle = {{YOLOv11}},
	url = {http://arxiv.org/abs/2410.17725},
	doi = {10.48550/arXiv.2410.17725},
	abstract = {This study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only Look Once) series of object detection models. We examine the models architectural innovations, including the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial Pyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) components, which contribute in improving the models performance in several ways such as enhanced feature extraction. The paper explores YOLOv11's expanded capabilities across various computer vision tasks, including object detection, instance segmentation, pose estimation, and oriented object detection (OBB). We review the model's performance improvements in terms of mean Average Precision (mAP) and computational efficiency compared to its predecessors, with a focus on the trade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11's versatility across different model sizes, from nano to extra-large, catering to diverse application needs from edge devices to high-performance computing environments. Our research provides insights into YOLOv11's position within the broader landscape of object detection and its potential impact on real-time computer vision applications.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Khanam, Rahima and Hussain, Muhammad},
	month = oct,
	year = {2024},
	note = {arXiv:2410.17725 
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{herfandi_real-time_2024,
	title = {Real-{Time} {Patient} {Indoor} {Health} {Monitoring} and {Location} {Tracking} with {Optical} {Camera} {Communications} on the {Internet} of {Medical} {Things}},
	volume = {14},
	doi = {10.3390/app14031153},
	abstract = {Optical Camera Communication (OCC) is an emerging technology that has attracted research interest in recent decades. Unlike previous communication technologies, OCC uses visible light as the medium to transmit data from receivers and cameras to receive the data. OCC has several advantages that can be capitalized in several implementations. However, the Internet of Things (IoT) has emerged as a technology with immense potential. Numerous research endeavors support the IoT’s prospective technology that can be implemented in various sectors, including the healthcare system. This study introduces a novel implementation of the Internet of Medical Things (IoMT) system, using OCC for real-time health monitoring and indoor location tracking. The innovative system uses standard closed-circuit television CCTV setups, integrating deep learning-based OCC to monitor multiple patients simultaneously, each represented by an LED matrix. The effectiveness of the system was demonstrated through two scenarios: the first involves dual transmitters and a single camera, highlighting real-time monitoring of vital health data; the second features a transmitter with dual cameras, focusing patient movement tracking across different camera fields of view. To accurately locate and track the position of LED arrays in the camera, the system used YOLO (You Only Look Once). Data are securely transmitted to an edge server and stored using the REST API, with a web interface providing real-time patient updates. This study highlights the potential of OCC in IoMT for advanced patient care and proposes future exploration in larger healthcare systems and other IoT domains.},
	journal = {Applied Sciences},
	author = {Herfandi, Herfandi and Sitanggang, Ones and Nasution, Muhammad and Nguyen, Huy and Jang, Yeong Min},
	month = jan,
	year = {2024},
	pages = {1153},
}

@misc{sapkota_comparing_2024,
	title = {Comparing {YOLO11} and {YOLOv8} for instance segmentation of occluded and non-occluded immature green fruits in complex orchard environment},
	url = {http://arxiv.org/abs/2410.19869},
	doi = {10.48550/arXiv.2410.19869},
	abstract = {This study conducted a comprehensive performance evaluation on YOLO11 and YOLOv8, the latest in the "You Only Look Once" (YOLO) series, focusing on their instance segmentation capabilities for immature green apples in orchard environments. YOLO11n-seg achieved the highest mask precision across all categories with a notable score of 0.831, highlighting its effectiveness in fruit detection. YOLO11m-seg and YOLO11l-seg excelled in non-occluded and occluded fruitlet segmentation with scores of 0.851 and 0.829, respectively. Additionally, YOLO11x-seg led in mask recall for all categories, achieving a score of 0.815, with YOLO11m-seg performing best for non-occluded immature green fruitlets at 0.858 and YOLOv8x-seg leading the occluded category with 0.800. In terms of mean average precision at a 50{\textbackslash}\% intersection over union (mAP@50), YOLO11m-seg consistently outperformed, registering the highest scores for both box and mask segmentation, at 0.876 and 0.860 for the "All" class and 0.908 and 0.909 for non-occluded immature fruitlets, respectively. YOLO11l-seg and YOLOv8l-seg shared the top box mAP@50 for occluded immature fruitlets at 0.847, while YOLO11m-seg achieved the highest mask mAP@50 of 0.810. Despite the advancements in YOLO11, YOLOv8n surpassed its counterparts in image processing speed, with an impressive inference speed of 3.3 milliseconds, compared to the fastest YOLO11 series model at 4.8 milliseconds, underscoring its suitability for real-time agricultural applications related to complex green fruit environments.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Sapkota, Ranjan and Karkee, Manoj},
	month = nov,
	year = {2024},
	note = {arXiv:2410.19869},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{alif_yolov11_2024,
	title = {{YOLOv11} for {Vehicle} {Detection}: {Advancements}, {Performance}, and {Applications} in {Intelligent} {Transportation} {Systems}},
	shorttitle = {{YOLOv11} for {Vehicle} {Detection}},
	url = {http://arxiv.org/abs/2410.22898},
	doi = {10.48550/arXiv.2410.22898},
	abstract = {Accurate vehicle detection is essential for the development of intelligent transportation systems, autonomous driving, and traffic monitoring. This paper presents a detailed analysis of YOLO11, the latest advancement in the YOLO series of deep learning models, focusing exclusively on vehicle detection tasks. Building upon the success of its predecessors, YOLO11 introduces architectural improvements designed to enhance detection speed, accuracy, and robustness in complex environments. Using a comprehensive dataset comprising multiple vehicle types-cars, trucks, buses, motorcycles, and bicycles we evaluate YOLO11's performance using metrics such as precision, recall, F1 score, and mean average precision (mAP). Our findings demonstrate that YOLO11 surpasses previous versions (YOLOv8 and YOLOv10) in detecting smaller and more occluded vehicles while maintaining a competitive inference time, making it well-suited for real-time applications. Comparative analysis shows significant improvements in the detection of complex vehicle geometries, further contributing to the development of efficient and scalable vehicle detection systems. This research highlights YOLO11's potential to enhance autonomous vehicle performance and traffic monitoring systems, offering insights for future developments in the field.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Alif, Mujadded Al Rabbani},
	month = oct,
	year = {2024},
	note = {arXiv:2410.22898},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{ackermann_automated_2022,
	title = {Automated segmentation of martensite-austenite islands in bainitic steel},
	volume = {191},
	doi = {10.1016/j.matchar.2022.112091},
	abstract = {So far, the qualitative and quantitative analysis of bainite has to be carried out by a metallography specialist and often causes ambiguities coming along with poor reproducibility. Possible reasons for a high variance in the description of bainite originate from different expert opinions given the high variety in the appearance of bainite in micrographs. In particular, the applied cooling regime and corresponding temperature gradients in the material dictate the evolving microstructure and its complexity. In the recent years, deep learning showed its potential to provide a robust and fast quantification of image data derived from learning large datasets. In order to unfold the potential of deep learning and facilitate its usage for the material science community, a deeper understanding on the role of data pre-processing is necessary to capture the influence of metallography images (and their complexity) on the learning process. In this study, the open-source detection and segmentation library Detectron2 (https://github.com/facebookresearch/detectron2) was used within a framework to quantify a crucial constituent in bainite - the martensite-austenite (M-A) islands - in electron microscopy images. We provide three bainite data sets with image data representing different cooling regimes and therefore different M-A characteristics. From segmentation results, the ratio of constituent to image size manifests as a crucial parameter during pre-processing affecting the accuracy of subsequently trained models.},
	journal = {Materials Characterization},
	author = {Ackermann, Marc and İren, Deniz and Wesselmecking, Sebastian and Shetty, Deekshith and Krupp, Ulrich},
	month = jul,
	year = {2022},
	pages = {112091},
}

@misc{yaseen_what_2024,
	title = {What is {YOLOv8}: {An} {In}-{Depth} {Exploration} of the {Internal} {Features} of the {Next}-{Generation} {Object} {Detector}},
	shorttitle = {What is {YOLOv8}},
	url = {http://arxiv.org/abs/2408.15857},
	doi = {10.48550/arXiv.2408.15857},
	abstract = {This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5. Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews YOLOv8's performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores YOLOv8's developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment. Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Yaseen, Muhammad},
	month = aug,
	year = {2024},
	note = {arXiv:2408.15857 
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{vera-yanez_vision-based_2023,
	title = {Vision-{Based} {Flying} {Obstacle} {Detection} for {Avoiding} {Midair} {Collisions}: {A} {Systematic} {Review}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2313-433X},
	shorttitle = {Vision-{Based} {Flying} {Obstacle} {Detection} for {Avoiding} {Midair} {Collisions}},
	url = {https://www.mdpi.com/2313-433X/9/10/194},
	doi = {10.3390/jimaging9100194},
	abstract = {This paper presents a systematic review of articles on computer-vision-based flying obstacle detection with a focus on midair collision avoidance. Publications from the beginning until 2022 were searched in Scopus, IEEE, ACM, MDPI, and Web of Science databases. From the initial 647 publications obtained, 85 were finally selected and examined. The results show an increasing interest in this topic, especially in relation to object detection and tracking. Our study hypothesizes that the widespread access to commercial drones, the improvements in single-board computers, and their compatibility with computer vision libraries have contributed to the increase in the number of publications. The review also shows that the proposed algorithms are mainly tested using simulation software and flight simulators, and only 26 papers report testing with physical flying vehicles. This systematic review highlights other gaps to be addressed in future work. Several identified challenges are related to increasing the success rate of threat detection and testing solutions in complex scenarios.},
	language = {en},
	number = {10},
	urldate = {2024-11-07},
	journal = {Journal of Imaging},
	author = {Vera-Yanez, Daniel and Pereira, António and Rodrigues, Nuno and Molina, José Pascual and García, Arturo S. and Fernández-Caballero, Antonio},
	month = oct,
	year = {2023},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {computer vision, midair collision, obstacle detection, systematic review},
	pages = {194},
}

@inproceedings{yang_object_2024,
	title = {Object {Detection} of {Visdrone} based on {Attention} {Mechanism} and {FasterNet}},
	url = {https://ieeexplore.ieee.org/document/10603457},
	doi = {10.1109/CVIDL62147.2024.10603457},
	abstract = {Object detection in Unmanned aerial vehicle (UAV) is an important foundation in various research fields. However, due to issues such as slow detection speed, more significant proportion of small targets, dense distribution, and instance overlap, drone target detection pose challenges. In this paper, an improved YOLOv8 architecture incorporating attention mechanism and FasterNet is proposed to enhance the detection performance of UVA images. First, FasterNet is applied in the backbone network of YOLOv8, which uses partial convolution (PConv) to better extract spatial features. Secondly, an attention mechanism called RepConv ShuffleNet (RCS)-based One-Shot Aggregation (RCS-OSA) module is employed to original neck which allows semantic information extraction. Experimental results on VisDrone dataset, indicate that the proposed method can effectively enhance the detection capability for drone targets, and upgrade the mean average precision by about 6 \%.},
	urldate = {2024-11-07},
	booktitle = {2024 5th {International} {Conference} on {Computer} {Vision}, {Image} and {Deep} {Learning} ({CVIDL})},
	author = {Yang, Linna and Wang, Yinchuan and Kong, Lei and Bai, Yun and Tao, Bo},
	month = apr,
	year = {2024},
	keywords = {Attention mechanisms, Autonomous aerial vehicles, Benchmark testing, FasterNet, Feature extraction, Object detection, Semantics, VisDrone dataset, Visualization, attention mechanism, object detection},
	pages = {257--261},
}

@misc{zhu_vision_2018,
	title = {Vision {Meets} {Drones}: {A} {Challenge}},
	shorttitle = {Vision {Meets} {Drones}},
	url = {http://arxiv.org/abs/1804.07437},
	doi = {10.48550/arXiv.1804.07437},
	abstract = {In this paper we present a large-scale visual object detection and tracking benchmark, named VisDrone2018, aiming at advancing visual understanding tasks on the drone platform. The images and video sequences in the benchmark were captured over various urban/suburban areas of 14 different cities across China from north to south. Specifically, VisDrone2018 consists of 263 video clips and 10,209 images (no overlap with video clips) with rich annotations, including object bounding boxes, object categories, occlusion, truncation ratios, etc. With intensive amount of effort, our benchmark has more than 2.5 million annotated instances in 179,264 images/video frames. Being the largest such dataset ever published, the benchmark enables extensive evaluation and investigation of visual analysis algorithms on the drone platform. In particular, we design four popular tasks with the benchmark, including object detection in images, object detection in videos, single object tracking, and multi-object tracking. All these tasks are extremely challenging in the proposed dataset due to factors such as occlusion, large scale and pose variation, and fast motion. We hope the benchmark largely boost the research and development in visual analysis on drone platforms.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Zhu, Pengfei and Wen, Longyin and Bian, Xiao and Ling, Haibin and Hu, Qinghua},
	month = apr,
	year = {2018},
	note = {arXiv:1804.07437},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{hsu_adaptive_2021,
	title = {Adaptive {Fusion} of {Multi}-{Scale} {YOLO} for {Pedestrian} {Detection}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/abstract/document/9507504},
	doi = {10.1109/ACCESS.2021.3102600},
	abstract = {Although pedestrian detection technology is constantly improving, pedestrian detection remains challenging because of the uncertainty and diversity of pedestrians in different scales and of occluded pedestrian modes. This study followed the common framework of single-shot object detection and proposed a divide-and-rule method to solve the aforementioned problems. The proposed model introduced a segmentation function that can split pedestrians who do not overlap in one image into two subimages. By using a network architecture, multiresolution adaptive fusion was performed on the output of all images and subimages to generate the final detection result. This study conducted an extensive evaluation of several challenging pedestrian detection data sets and finally proved the effectiveness of the proposed model. In particular, the proposed model achieved the most advanced performance on data sets from Visual Object Classes 2012 (VOC 2012), the French Institute for Research in Computer Science and Automation, and the Swiss Federal Institute of Technology in Zurich and obtained the most competitive results in a triple-width VOC 2012 experiment carefully designed by the present study.},
	urldate = {2024-11-04},
	journal = {IEEE Access},
	author = {Hsu, Wei-Yen and Lin, Wen-Yen},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Adaptation models, Deep learning, Feature extraction, Image resolution, Image segmentation, Neural networks, Object detection, Pedestrian detection, adaptive fusion, multi-scale YOLO},
	pages = {110063--110073},
}

@inproceedings{doshi_road_2020,
	title = {Road {Damage} {Detection} using {Deep} {Ensemble} {Learning}},
	url = {https://ieeexplore.ieee.org/abstract/document/9377774},
	doi = {10.1109/BigData50022.2020.9377774},
	abstract = {Road damage detection is critical for the maintenance of a road, which traditionally has been performed using expensive high-performance sensors. With the recent advances in technology, especially in computer vision, it is now possible to detect and categorize different types of road damages, which can facilitate efficient maintenance and resource management. In this work, an ensemble model for efficient detection and classification of road damages, which has been submitted to the IEEE BigData Cup Challenge 2020. The solution utilizes a state-of-the-art object detector known as You Only Look Once (YOLO-v4), which is trained on images of various types of road damages from Czech, Japan and India. The ensemble approach was extensively tested with several different model versions and it was able to achieve an F1 score of 0.628 on the test 1 dataset and 0.6358 on the test 2 dataset.},
	urldate = {2024-11-03},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Doshi, Keval and Yilmaz, Yasin},
	month = dec,
	year = {2020},
	keywords = {Big Data, Data models, Detectors, Maintenance engineering, Predictive models, Roads, Task analysis, convolution neural network, ensemble models, image classification, object detection, road damage},
	pages = {5540--5544},
}

@inproceedings{dikairono_multiple_2024,
	title = {Multiple {Obstacles} {Avoidance} {Path} {Planning} for {Unmanned} {Surface} {Vehicle}},
	url = {https://ieeexplore.ieee.org/document/10668174},
	doi = {10.1109/ISITIA63062.2024.10668174},
	abstract = {Water surface missions often use Unmanned Surface Vehicle (USV) for various missions above the water surface, from special missions such as liquefying victims to regular missions such as shallow sea depth mapping to produce bathymetric maps. As an autonomous vehicle, USV must have the ability to avoid obstacles. This paper explains a multi-obstacle avoidance path planning system in an USV. Along with waypoint navigation, the addition of Velodyne VLP-16 LiDAR technology and cameras enables real-time object detection and environmental mapping. The proposed method simultaneously combines waypoint input, object detection, and position mapping, effectively creating a dynamic obstacle database. By utilizing this database, the USV can calculate collision-free paths in real time, ensuring safe navigation. We validate our system on the Nala Proteus hull, successfully guiding it through a 20-meter track with three obstacles without colliding, thus demonstrating the efficiency and effectiveness of this system.},
	urldate = {2024-11-03},
	booktitle = {2024 {International} {Seminar} on {Intelligent} {Technology} and {Its} {Applications} ({ISITIA})},
	author = {Dikairono, Rudy and {Muhtadin} and {Widyastuti} and Hakim, M. Lukman and Arifianto, Dhany},
	month = jul,
	year = {2024},
	note = {ISSN: 2769-5492},
	keywords = {Cameras, Databases, Laser radar, Multiple Obstacles, Navigation, Path Planning, Real-time systems, Sea surface, Unmanned Surface Vehicle (USVs), YOLO},
	pages = {611--615},
}

@inproceedings{xu_gfspp-yolo_2023,
	title = {{GFSPP}-{YOLO}: {A} {Light} {YOLO} {Model} {Based} on {Group} {Fast} {Spatial} {Pyramid} {Pooling}},
	shorttitle = {{GFSPP}-{YOLO}},
	url = {https://ieeexplore.ieee.org/document/10393445},
	doi = {10.1109/ICICN59530.2023.10393445},
	abstract = {The YOLO object detection model for PC environments is widely used in computer vision due to its high accuracy and good real-time performance. However, when faced with the embedded environment of mobile devices, the use of YOLO models in mobile devices is still challenging due to the large computational requirements and memory consumption. To address these issues, this paper proposes a lightweight YOLO model based on grouped fast spatial pyramidal pooling. Different from the existing YOLOv5 model, firstly, at the end of the backbone network, the receptive field is expanded using the ideas of CSPNet and group convolution to build a group fast spatial pyramidal pooling structure GFSPP to avoid false and missed detections caused by image distortion; and a CBAM attention mechanism is introduced in the backbone network to improve the characterization of network features. Secondly, the slim neck paradigm combined with the lightweight convolutional module GhostConv is used in the neck network to compress the network structure. Finally, migration learning techniques are used to further improve the detection performance of the model. Experimental results show that the GFSPP-YOLO model proposed in this paper reduces the complexity and parameter costs by 10\% and 3.5\% respectively compared to the traditional YOLOv5s model on the PASCAL VOC2007+12 dataset, while the mAP0.5 is improved by 2\%, making the model in this paper more suitable for applications in embedded environments of mobile terminals.},
	urldate = {2024-11-03},
	booktitle = {2023 {IEEE} 11th {International} {Conference} on {Information}, {Communication} and {Networks} ({ICICN})},
	author = {Xu, Shaojie and Ji, Yujiao and Wang, Guangcheng and Jin, Lei and Wang, Han},
	month = aug,
	year = {2023},
	keywords = {Computational modeling, Feature extraction, Memory management, Performance evaluation, Power demand, Transfer learning, YOLO, convolutional neural network, group fast spatial pyramid pooling, lightweight YOLO, object detection},
	pages = {733--738},
}

@article{loquercio_dronet_2018,
	title = {{DroNet}: {Learning} to {Fly} by {Driving}},
	volume = {3},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{DroNet}},
	url = {http://ieeexplore.ieee.org/document/8264734/},
	doi = {10.1109/LRA.2018.2795643},
	abstract = {Civilian drones are soon expected to be used in a wide variety of tasks, such as aerial surveillance, delivery, or monitoring of existing architectures. Nevertheless, their deployment in urban environments has so far been limited. Indeed, in unstructured and highly dynamic scenarios, drones face numerous challenges to navigate autonomously in a feasible and safe way. In contrast to traditional “map-localize-plan” methods, this paper explores a data-driven approach to cope with the above challenges. To accomplish this, we propose DroNet: a convolutional neural network that can safely drive a drone through the streets of a city. Designed as a fast 8-layers residual network, DroNet produces two outputs for each single input image: a steering angle to keep the drone navigating while avoiding obstacles, and a collision probability to let the UAV recognize dangerous situations and promptly react to them. The challenge is however to collect enough data in an unstructured outdoor environment such as a city. Clearly, having an expert pilot providing training trajectories is not an option given the large amount of data required and, above all, the risk that it involves for other vehicles or pedestrians moving in the streets. Therefore, we propose to train a UAV from data collected by cars and bicycles, which, already integrated into the urban environment, would not endanger other vehicles and pedestrians. Although trained on city streets from the viewpoint of urban vehicles, the navigation policy learned by DroNet is highly generalizable. Indeed, it allows a UAV to successfully ﬂy at relative high altitudes and even in indoor environments, such as parking lots and corridors. To share our ﬁndings with the robotics community, we publicly release all our datasets, code, and trained networks.},
	language = {en},
	number = {2},
	urldate = {2024-11-03},
	journal = {IEEE Robotics and Automation Letters},
	author = {Loquercio, Antonio and Maqueda, Ana I. and del-Blanco, Carlos R. and Scaramuzza, Davide},
	month = apr,
	year = {2018},
	pages = {1088--1095},
}

@inproceedings{sharma_traffic_2023,
	title = {Traffic {Sign} {Board} {Prediction} {Using} {Ensemble} {Model} of {YOLOv8} {And} {Detectron2}},
	url = {https://ieeexplore.ieee.org/abstract/document/10465993},
	doi = {10.1109/ICAICCIT60255.2023.10465993},
	abstract = {Road safety is greatly enhanced and smart decision-making for self-driving cars and intelligent modes of transportation is made possible by the detection and classification of traffic signs. In order to attain cutting-edge performance, this study suggests an ensemble model for traffic sign board prediction that combines YOLOv8 with Detectron2. While Detectron2 offers accurate object boundaries using instance segmentation, YOLOv8 is renowned for its quickness and precision in object identification. The ensemble technique improves the overall accuracy and resilience of the traffic sign board prediction system by taking advantage of the complimentary characteristics of both models. Benchmark datasets like the Belgian Traffic Sign Dataset and the German Traffic Sign Recognition Benchmark are the subject of extensive experiments. The YOLOv8 and Detectron2 individual models are outperformed by the ensemble model about of accuracy, precision, recall, and F1 score. In addition, the ensemble model performs better in difficult settings like occlusions and various lighting levels. The results demonstrate the efficiency of ensemble models for traffic sign board prediction as well as their potential to improve both intelligent transportation systems and road safety.},
	urldate = {2024-11-03},
	booktitle = {2023 {International} {Conference} on {Advances} in {Computation}, {Communication} and {Information} {Technology} ({ICAICCIT})},
	author = {Sharma, Abhilasha and Ranjan, Prabhat},
	month = nov,
	year = {2023},
	keywords = {Autonomous automobiles, Benchmark testing, Computational modeling, Detectron2, Predictive models, Road safety, System performance, Traffic sign board prediction, Transformers, YOLOv8, ensemble model, highway safety, smart transportation systems},
	pages = {378--384},
}

@misc{reis_real-time_2024,
	title = {Real-{Time} {Flying} {Object} {Detection} with {YOLOv8}},
	url = {http://arxiv.org/abs/2305.09972},
	doi = {10.48550/arXiv.2305.09972},
	abstract = {This paper presents a generalized model for real-time detection of flying objects that can be used for transfer learning and further research, as well as a refined model that achieves state-of-the-art results for flying object detection. We achieve this by training our first (generalized) model on a data set containing 40 different classes of flying objects, forcing the model to extract abstract feature representations. We then perform transfer learning with these learned parameters on a data set more representative of real world environments (i.e. higher frequency of occlusion, very small spatial sizes, rotations, etc.) to generate our refined model. Object detection of flying objects remains challenging due to large variances of object spatial sizes/aspect ratios, rate of speed, occlusion, and clustered backgrounds. To address some of the presented challenges while simultaneously maximizing performance, we utilize the current state-of-the-art single-shot detector, YOLOv8, in an attempt to find the best trade-off between inference speed and mean average precision (mAP). While YOLOv8 is being regarded as the new state-of-the-art, an official paper has not been released as of yet. Thus, we provide an in-depth explanation of the new architecture and functionality that YOLOv8 has adapted. Our final generalized model achieves a mAP50 of 79.2\%, mAP50-95 of 68.5\%, and an average inference speed of 50 frames per second (fps) on 1080p videos. Our final refined model maintains this inference speed and achieves an improved mAP50 of 99.1\% and mAP50-95 of 83.5\%},
	urldate = {2024-11-03},
	publisher = {arXiv},
	author = {Reis, Dillon and Kupec, Jordan and Hong, Jacqueline and Daoudi, Ahmad},
	month = may,
	year = {2024},
	note = {arXiv:2305.09972 
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{rahman_real-time_2023,
	title = {Real-{Time} {Obstacle} {Detection} with {YOLOv8} in a {WSN} {Using} {UAV} {Aerial} {Photography}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/9/10/216},
	doi = {10.3390/jimaging9100216},
	abstract = {Nowadays, wireless sensor networks (WSNs) have a significant and long-lasting impact on numerous fields that affect all facets of our lives, including governmental, civil, and military applications. WSNs contain sensor nodes linked together via wireless communication links that need to relay data instantly or subsequently. In this paper, we focus on unmanned aerial vehicle (UAV)-aided data collection in wireless sensor networks (WSNs), where multiple UAVs collect data from a group of sensors. The UAVs may face some static or moving obstacles (e.g., buildings, trees, static or moving vehicles) in their traveling path while collecting the data. In the proposed system, the UAV starts and ends the data collection tour at the base station, and, while collecting data, it captures images and videos using the UAV aerial camera. After processing the captured aerial images and videos, UAVs are trained using a YOLOv8-based model to detect obstacles in their traveling path. The detection results show that the proposed YOLOv8 model performs better than other baseline algorithms in different scenarios—the F1 score of YOLOv8 is 96\% in 200 epochs.},
	language = {en},
	number = {10},
	urldate = {2024-11-03},
	journal = {Journal of Imaging},
	author = {Rahman, Shakila and Rony, Jahid Hasan and Uddin, Jia and Samad, Md Abdus},
	month = oct,
	year = {2023},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {UAV aerial photography, YOLOv8, obstacle detection, unmanned aerial vehicles (UAVs), wireless sensor networks (WSNs)},
	pages = {216},
}

@misc{wang_yolov7_2022,
	title = {{YOLOv7}: {Trainable} bag-of-freebies sets new state-of-the-art for real-time object detectors},
	shorttitle = {{YOLOv7}},
	url = {http://arxiv.org/abs/2207.02696},
	doi = {10.48550/arXiv.2207.02696},
	abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/WongKinYiu/yolov7.},
	urldate = {2024-11-03},
	publisher = {arXiv},
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
	month = jul,
	year = {2022},
	note = {arXiv:2207.02696},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{bigazzi_fast_2022,
	title = {Fast {Obstacle} {Detection} {System} for {UAS} {Based} on {Complementary} {Use} of {Radar} and {Stereoscopic} {Camera}},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-446X},
	url = {https://www.mdpi.com/2504-446X/6/11/361},
	doi = {10.3390/drones6110361},
	abstract = {Autonomous unmanned aerial systems (UAS) are having an increasing impact in the scientific community. One of the most challenging problems in this research area is the design of robust real-time obstacle detection and avoidance systems. In the automotive field, applications of obstacle detection systems combining radar and vision sensors are common and widely documented. However, these technologies are not currently employed in the UAS field due to the major complexity of the flight scenario, especially in urban environments. In this paper, a real-time obstacle-detection system based on the use of a 77 GHz radar and a stereoscopic camera is proposed for use in small UASs. The resulting system is capable of detecting obstacles in a broad spectrum of environmental conditions. In particular, the vision system guarantees a high resolution for short distances, while the radar has a lower resolution but can cover greater distances, being insensitive to poor lighting conditions. The developed hardware and software architecture and the related obstacle-detection algorithm are illustrated within the European project AURORA. Experimental results carried out employing a small UAS show the effectiveness of the obstacle detection system and of a simple avoidance strategy during several autonomous missions on a test site.},
	language = {en},
	number = {11},
	urldate = {2024-11-03},
	journal = {Drones},
	author = {Bigazzi, Luca and Miccinesi, Lapo and Boni, Enrico and Basso, Michele and Consumi, Tommaso and Pieraccini, Massimiliano},
	month = nov,
	year = {2022},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {UAS, autonomous navigation, computer vision, obstacle detection and avoidance, radar},
	pages = {361},
}

@article{zhao_object_2019,
	title = {Object {Detection} {With} {Deep} {Learning}: {A} {Review}},
	volume = {30},
	issn = {2162-2388},
	shorttitle = {Object {Detection} {With} {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/document/8627998/?arnumber=8627998},
	doi = {10.1109/TNNLS.2018.2876865},
	abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.},
	number = {11},
	urldate = {2024-09-29},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
	month = nov,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Computer architecture, Deep learning, Feature extraction, Neural networks, Object detection, Task analysis, Training, neural network, object detection},
	pages = {3212--3232},
}

@article{yan_inclined_2024,
	title = {Inclined {Obstacle} {Recognition} and {Ranging} {Method} in {Farmland} {Based} on {Improved} {YOLOv8}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2032-6653},
	url = {https://www.mdpi.com/2032-6653/15/3/104},
	doi = {10.3390/wevj15030104},
	abstract = {Unmanned tractors under ploughing conditions suffer from body tilting, violent shaking and limited hardware resources, which can reduce the detection accuracy of unmanned tractors for field obstacles. We optimize the YOLOv8 model in three aspects: improving the accuracy of detecting tilted obstacles, computational reduction, and adding a visual ranging mechanism. By introducing Funnel ReLU, a self-constructed inclined obstacle dataset, and embedding an SE attention mechanism, these three methods improve detection accuracy. By using MobileNetv2 and Bi FPN, computational reduction, and adding camera ranging instead of LIDAR ranging, the hardware cost is reduced. After completing the model improvement, comparative tests and real-vehicle validation are carried out, and the validation results show that the average detection accuracy of the improved model reaches 98.84\% of the mAP value, which is 2.34\% higher than that of the original model. The computation amount of the same image is reduced from 2.35 billion floating-point computations to 1.28 billion, which is 45.53\% less than the model computation amount. The monitoring frame rate during the movement of the test vehicle reaches 67 FPS, and the model meets the performance requirements of unmanned tractors under normal operating conditions.},
	language = {en},
	number = {3},
	urldate = {2024-09-29},
	journal = {World Electric Vehicle Journal},
	author = {Yan, Xianghai and Chen, Bingxin and Liu, Mengnan and Zhao, Yifan and Xu, Liyou},
	month = mar,
	year = {2024},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {algorithm improvement, camera ranging, dedicated dataset, inclined working condition, unmanned tractor},
	pages = {104},
}

@article{wenning_anomaly_2022,
	title = {Anomaly detection as vision-based obstacle detection for vehicle automation in industrial environment},
	volume = {2},
	issn = {2813-0359},
	url = {https://www.frontiersin.org/journals/manufacturing-technology/articles/10.3389/fmtec.2022.918343/full},
	doi = {10.3389/fmtec.2022.918343},
	abstract = {{\textless}p{\textgreater}Nowadays, produced cars are equipped with mechatronical actuators as well as with a wide range of sensors in order to realize driver assistance functions. These components could enable cars’ automation at low speeds on company premises, although autonomous driving in public traffic is still facing technical and legal challenges. For automating vehicles in an industrial environment a reliable obstacle detection system is required. State-of-the-art solution for protective devices in Automated Guided Vehicles is the distance measuring laser scanner. Since laser scanners are not basic equipment of today’s cars in contrast to monocameras mounted behind the windscreen, we develop a computer vision algorithm that is able to detect obstacles in camera images reliably. Therefore, we make use of our well-known operational design domain by teaching an anomaly detection how the vehicle path should look like. The result is an anomaly detection algorithm that consists of a pre-trained feature extractor and a shallow classifier, modelling the probability of occurrence. We record a data set of a real industrial environment and show a robust classifier after training the algorithm with images of only one run. The performance as an obstacle detection is on par with a semantic segmentation, but requires a fraction of the training data and no labeling.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-09-29},
	journal = {Frontiers in Manufacturing Technology},
	author = {Wenning, Marius and Adlon, Tobias and Burggräf, Peter},
	month = aug,
	year = {2022},
	note = {Publisher: Frontiers},
	keywords = {Anomaly detection (AD), Automated guided vehicle, Autonomous transport, Obstacle detection, obstacle avoidance},
}

@article{wang_efficient_2018,
	title = {Efficient {Rail} {Area} {Detection} {Using} {Convolutional} {Neural} {Network}},
	volume = {6},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8550680/?arnumber=8550680},
	doi = {10.1109/ACCESS.2018.2883704},
	abstract = {Rail area detection is essential in active obstacle perception system of the train. This paper presents an efficient rail area detection method based on the convolutional neural network (CNN). The proposed method is divided into two main parts: extraction of the rail area and further optimization. First, a CNN architecture is established to achieve accurate rail area detection, enabling the pixel-level classification of the rail area. It is notable that the main improvement of our architecture is dilated cascade connection and cascade sampling. Second, an improved polygon fitting method is applied to optimize the contour of the extracted rail area and, thus, obtains a more elegant outline of the rail region. As shown by the experimental results, the excellent accuracy is obtained by using our method, i.e., 98.46\% mean intersection-over-union and 99.15\% mean pixel accuracy on the BH-rail-dataset, and verified the applicability of our detection method in a large-scale traffic scene video frames of Beijing metro Yanfang line and Shanghai metro line 6.},
	urldate = {2024-09-29},
	journal = {IEEE Access},
	author = {Wang, Zhangyu and Wu, Xinkai and Yu, Guizhen and Li, Mingxing},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {Cameras, Convolution, Convolutional neural networks, Feature extraction, Network architecture, Rail area detection, Rail transportation, Rails, convolutional neural network, polygon fitting},
	pages = {77656--77664},
}

@inproceedings{wadhwa_comparison_2023,
	title = {Comparison of {YOLOv8} and {Detectron2} on {Crowd} {Counting} techniques},
	url = {https://ieeexplore.ieee.org/document/10391466/?arnumber=10391466},
	doi = {10.1109/ISAS60782.2023.10391466},
	abstract = {With the increase of security and surveillance cameras in public spaces has also created a demand for analyzing them and Crowd counting has gained a lot of popularity in recent times as it is viewed as a one of the major components for ensuring public safety and monitoring as it is a very laborious task for a person to sit in front of a security footage. It can be a very challenging task to create a dataset for training an algorithm and laborious data annotation is required for training and testing of the algorithms. This research paper discusses the 2 popular approaches of crowd counting in use today and examines their key features. The techniques used here are YOLOvS and Detectron2 which can be used for object and image detection and segmentation. YOLO stands for”You Only Look Once” and it is a popular algorithm as it is suitable for a wide variety of object detection, image segmentation and image classification tasks.Detectron2 is a Facebook AI Research (FAIR)’s library for object detection and segmentation in images. In our research, we found YOLOvS performance is better than detectron2. Detectron2 has been performed better than YOLOvS in terms of overall accuracy, but it has been observed on the basis of the experiments conducted above that YOLOvS is able to detect more objects than Detectron},
	urldate = {2024-09-29},
	booktitle = {2023 7th {International} {Symposium} on {Innovative} {Approaches} in {Smart} {Technologies} ({ISAS})},
	author = {Wadhwa, Mehul and Choudhury, Tanupriya and Raj, Gaurav and Patni, Jagdish Chandra},
	month = nov,
	year = {2023},
	keywords = {Computer Vision, Crowd Counting, Detectron2, Image segmentation, Public security, Real-time systems, Social networking (online), Surveillance, Training, YOLO, YOLOv8},
	pages = {1--6},
}

@inproceedings{varghese_yolov8_2024,
	title = {{YOLOv8}: {A} {Novel} {Object} {Detection} {Algorithm} with {Enhanced} {Performance} and {Robustness}},
	shorttitle = {{YOLOv8}},
	url = {https://ieeexplore.ieee.org/document/10533619},
	doi = {10.1109/ADICS58448.2024.10533619},
	abstract = {In recent years, the You Only Look Once (YOLO) series of object detection algorithms have garnered significant attention for their speed and accuracy in real-time applications. This paper presents YOLOv8, a novel object detection algorithm that builds upon the advancements of previous iterations, aiming to further enhance performance and robustness. Inspired by the evolution of YOLO architectures from YOLOv1 to YOLOv7, as well as insights from comparative analyses of models like YOLOv5 and YOLOv6, YOLOv8 incorporates key innovations to achieve optimal speed and accuracy. Leveraging attention mechanisms and dynamic convolution, YOLOv8 introduces improvements specifically tailored for small object detection, addressing challenges highlighted in YOLOv7. Additionally, the integration of voice recognition techniques enhances the algorithm's capabilities for video-based object detection, as demonstrated in YOLOv7. The proposed algorithm undergoes rigorous evaluation against state-of-the-art benchmarks, showcasing superior performance in terms of both detection accuracy and computational efficiency. Experimental results on various datasets confirm the effectiveness of YOLOv8 across diverse scenarios, further validating its suitability for real-world applications. This paper contributes to the ongoing advancements in object detection research by presenting YOLOv8 as a versatile and high-performing algorithm, poised to address the evolving needs of computer vision systems.},
	urldate = {2024-10-02},
	booktitle = {2024 {International} {Conference} on {Advances} in {Data} {Engineering} and {Intelligent} {Computing} {Systems} ({ADICS})},
	author = {Varghese, Rejin and M., Sambath},
	month = apr,
	year = {2024},
	keywords = {Benchmark testing, Computational Efficiency, Computer Vision Systems, Computer vision, Heuristic algorithms, Object Detection, Performance Enhancement, Performance evaluation, Robustness, Speech recognition, Technological innovation, YOLO, YOLOv8},
	pages = {1--6},
}

@article{son_teacherstudent_2024,
	title = {Teacher–{Student} {Model} {Using} {Grounding} {DINO} and {You} {Only} {Look} {Once} for {Multi}-{Sensor}-{Based} {Object} {Detection}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/14/6/2232},
	doi = {10.3390/app14062232},
	abstract = {Object detection is a crucial research topic in the fields of computer vision and artificial intelligence, involving the identification and classification of objects within images. Recent advancements in deep learning technologies, such as YOLO (You Only Look Once), Faster-R-CNN, and SSDs (Single Shot Detectors), have demonstrated high performance in object detection. This study utilizes the YOLOv8 model for real-time object detection in environments requiring fast inference speeds, specifically in CCTV and automotive dashcam scenarios. Experiments were conducted using the ‘Multi-Image Identical Situation and Object Identification Data’ provided by AI Hub, consisting of multi-image datasets captured in identical situations using CCTV, dashcams, and smartphones. Object detection experiments were performed on three types of multi-image datasets captured in identical situations. Despite the utility of YOLO, there is a need for performance improvement in the AI Hub dataset. Grounding DINO, a zero-shot object detector with a high mAP performance, is employed. While efficient auto-labeling is possible with Grounding DINO, its processing speed is slower than YOLO, making it unsuitable for real-time object detection scenarios. This study conducts object detection experiments using publicly available labels and utilizes Grounding DINO as a teacher model for auto-labeling. The generated labels are then used to train YOLO as a student model, and performance is compared and analyzed. Experimental results demonstrate that using auto-generated labels for object detection does not lead to degradation in performance. The combination of auto-labeling and manual labeling significantly enhances performance. Additionally, an analysis of datasets containing data from various devices, including CCTV, dashcams, and smartphones, reveals the impact of different device types on the recognition accuracy for distinct devices. Through Grounding DINO, this study proves the efficacy of auto-labeling technology in contributing to efficiency and performance enhancement in the field of object detection, presenting practical applicability.},
	language = {en},
	number = {6},
	urldate = {2024-10-03},
	journal = {Applied Sciences},
	author = {Son, Jinhwan and Jung, Heechul},
	month = jan,
	year = {2024},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {auto-labeling, computer vision, deep learning, object detection},
	pages = {2232},
}

@article{turay_toward_2022,
	title = {Toward {Performing} {Image} {Classification} and {Object} {Detection} {With} {Convolutional} {Neural} {Networks} in {Autonomous} {Driving} {Systems}: {A} {Survey}},
	volume = {10},
	issn = {2169-3536},
	shorttitle = {Toward {Performing} {Image} {Classification} and {Object} {Detection} {With} {Convolutional} {Neural} {Networks} in {Autonomous} {Driving} {Systems}},
	url = {https://ieeexplore.ieee.org/document/9696317/?arnumber=9696317},
	doi = {10.1109/ACCESS.2022.3147495},
	abstract = {Nowadays Convolutional Neural Networks (CNNs) are being employed in a wide range of industrial technologies for a variety of sectors, such as medical, automotive, aviation, agriculture, space, etc. This paper reviews the state-of-the-art in both the field of CNNs for image classification and object detection and Autonomous Driving Systems (ADSs) in a synergetic way. Layer-based details of CNNs along with parameter and floating-point operation number calculations are outlined. Using an evolutionary approach, the majority of the outstanding image classification CNNs, published in the open literature, is introduced with a focus on their accuracy performance, parameter number, model size, and inference speed, highlighting the progressive developments in convolutional operations. Results of a novel investigation of the convolution types and operations commonly used in CNNs are presented, including a timing analysis aimed at assessing their impact on CNN performance. This extensive experimental study provides new insight into the behaviour of each convolution type in terms of training time, inference time, and layer level decomposition. Building blocks for CNN-based object detection are also discussed, such as backbone networks and baseline types, and then representative state-of-the-art designs are outlined. Experimental results from the literature are summarised for each of the reviewed models. This is followed by an overview of recent ADSs related works and current industry activities, aiming to bridge academic research and industry practice on CNNs and ADSs. Design approaches targeted at solving problems of automakers in achieving real-time implementations are also proposed based on a discussion of design constraints, human vs. machine evaluations and trade-off analysis of accuracy vs. size. Current technologies, promising directions, and expectations from the literature on ADSs are introduced including a comprehensive trade-off analysis from a human-machine perspective.},
	urldate = {2024-09-29},
	journal = {IEEE Access},
	author = {Turay, Tolga and Vladimirova, Tanya},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {Autonomous vehicles, Convolution, Convolutional neural networks, Image classification, Laser radar, Object detection, Real-time systems, Task analysis, computer vision, convolution, convolutional neural networks, embedded systems, image classification, object detection},
	pages = {14076--14119},
}

@inproceedings{sarda_object_2021,
	title = {Object {Detection} for {Autonomous} {Driving} using {YOLO} [{You} {Only} {Look} {Once}] algorithm},
	url = {https://ieeexplore.ieee.org/document/9388577/?arnumber=9388577},
	doi = {10.1109/ICICV50876.2021.9388577},
	abstract = {The field of autonomous driving is going to be the face of the automobile industry very soon. The number of accidents that take place because of human error currently is very high and it can be slashed to a huge extent with the advent of autonomous driving. One of the primary prerequisites and a huge part of autonomous driving is dependent on object detection through computer vision, this paper aims at aiding towards the field of autonomous driving by helping detect objects with the use of deep learning algorithms. Research work used state-of-the-art algorithm YOLO (you only look once) to detect different objects that appear on the road and classified into the category that they belong to with the help of bounding boxes. The weights of the YOLO v4 is utilized to custom train our model to detect the objects and the data will be collected from the open images dataset using its OIDv4 toolkit.},
	urldate = {2024-09-29},
	booktitle = {2021 {Third} {International} {Conference} on {Intelligent} {Communication} {Technologies} and {Virtual} {Mobile} {Networks} ({ICICV})},
	author = {Sarda, Abhishek and Dixit, Shubhra and Bhan, Anupama},
	month = feb,
	year = {2021},
	keywords = {Autonomous vehicles, Classification algorithms, Deep learning, Faces, Industries, Object detection, Roads, YOLO, autonomous vehicles, computer vision, object detection},
	pages = {1370--1374},
}

@article{said_obstacle_2023,
	title = {Obstacle {Detection} {System} for {Navigation} {Assistance} of {Visually} {Impaired} {People} {Based} on {Deep} {Learning} {Techniques}},
	volume = {23},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10255983/},
	doi = {10.3390/s23115262},
	abstract = {Visually impaired people seek social integration, yet their mobility is restricted. They need a personal navigation system that can provide privacy and increase their confidence for better life quality. In this paper, based on deep learning and neural architecture search (NAS), we propose an intelligent navigation assistance system for visually impaired people. The deep learning model has achieved significant success through well-designed architecture. Subsequently, NAS has proved to be a promising technique for automatically searching for the optimal architecture and reducing human efforts for architecture design. However, this new technique requires extensive computation, limiting its wide use. Due to its high computation requirement, NAS has been less investigated for computer vision tasks, especially object detection. Therefore, we propose a fast NAS to search for an object detection framework by considering efficiency. The NAS will be used to explore the feature pyramid network and the prediction stage for an anchor-free object detection model. The proposed NAS is based on a tailored reinforcement learning technique. The searched model was evaluated on a combination of the Coco dataset and the Indoor Object Detection and Recognition (IODR) dataset. The resulting model outperformed the original model by 2.6\% in average precision (AP) with acceptable computation complexity. The achieved results proved the efficiency of the proposed NAS for custom object detection.},
	number = {11},
	urldate = {2024-09-29},
	journal = {Sensors (Basel, Switzerland)},
	author = {Said, Yahia and Atri, Mohamed and Albahar, Marwan Ali and Ben Atitallah, Ahmed and Alsariera, Yazan Ahmad},
	month = jun,
	year = {2023},
	pmid = {37299996},
	pmcid = {PMC10255983},
	pages = {5262},
}

@article{safaldin_improved_2024,
	title = {An {Improved} {YOLOv8} to {Detect} {Moving} {Objects}},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10508365/?arnumber=10508365},
	doi = {10.1109/ACCESS.2024.3393835},
	abstract = {Deep learning has revolutionized object detection, with YOLO (You Only Look Once) leading in real-time accuracy. However, detecting moving objects in visual streams presents distinct challenges. This paper proposes a refined YOLOv8 object detection model, emphasizing motion-specific detections in varied visual contexts. Through tailored preprocessing and architectural adjustments, we heighten the model’s sensitivity to object movements. Rigorous testing against KITTI, LASIESTA, PESMOD, and MOCS benchmark datasets revealed that the modified YOLOv8 outperforms the state-of-the-art detection models, especially in environments with significant movement. Specifically, our model achieved an accuracy of 90\%, a mean Average Precision (mAP) of 90\%, and maintained a processing speed of 30 frames per second (FPS), with an Intersection over Union (IoU) score of 80\%. This paper offers a detailed insight into object trajectories, proving invaluable in areas like security, traffic management, and film analysis where motion understanding is critical. As the importance of dynamic scene interpretation grows in artificial intelligence and computer vision, the proposed enhanced YOLOv8 detection model highlights the potential of specialized object detection and underscores the significance of our findings in the evolving field of object detection.},
	urldate = {2024-09-29},
	journal = {IEEE Access},
	author = {Safaldin, Mukaram and Zaghden, Nizar and Mejdoub, Mahmoud},
	year = {2024},
	note = {Conference Name: IEEE Access},
	keywords = {Computer architecture, Deep learning, Detectors, Feature extraction, Object recognition, Real-time systems, Task analysis, YOLO, localization, object detection, segmentation},
	pages = {59782--59806},
}

@article{ristic-durrant_review_2021,
	title = {A {Review} of {Vision}-{Based} {On}-{Board} {Obstacle} {Detection} and {Distance} {Estimation} in {Railways}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/10/3452},
	doi = {10.3390/s21103452},
	abstract = {This paper provides a review of the literature on vision-based on-board obstacle detection and distance estimation in railways. Environment perception is crucial for autonomous detection of obstacles in a vehicle’s surroundings. The use of on-board sensors for road vehicles for this purpose is well established, and advances in Artificial Intelligence and sensing technologies have motivated significant research and development in obstacle detection in the automotive field. However, research and development on obstacle detection in railways has been less extensive. To the best of our knowledge, this is the first comprehensive review of on-board obstacle detection methods for railway applications. This paper reviews currently used sensors, with particular focus on vision sensors due to their dominant use in the field. It then discusses and categorizes the methods based on vision sensors into methods based on traditional Computer Vision and methods based on Artificial Intelligence.},
	language = {en},
	number = {10},
	urldate = {2024-10-02},
	journal = {Sensors},
	author = {Ristić-Durrant, Danijela and Franke, Marten and Michels, Kai},
	month = jan,
	year = {2021},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {AI-based vision, autonomous obstacle detection, on-board vision sensors, railways, traditional computer vision},
	pages = {3452},
}

@misc{ren_grounding_2024,
	title = {Grounding {DINO} 1.5: {Advance} the "{Edge}" of {Open}-{Set} {Object} {Detection}},
	shorttitle = {Grounding {DINO} 1.5},
	url = {http://arxiv.org/abs/2405.10300},
	abstract = {This paper introduces Grounding DINO 1.5, a suite of advanced open-set object detection models developed by IDEA Research, which aims to advance the “Edge”1 of open-set object detection. The suite encompasses two models: Grounding DINO 1.5 Pro, a high-performance model designed for stronger generalization capability across a wide range of scenarios, and Grounding DINO 1.5 Edge, an efficient model optimized for faster speed demanded in many applications requiring edge deployment. The Grounding DINO 1.5 Pro model advances its predecessor by scaling up the model architecture, integrating an enhanced vision backbone, and expanding the training dataset to over 20 million images with grounding annotations, thereby achieving a richer semantic understanding. The Grounding DINO 1.5 Edge model, while designed for efficiency with reduced feature scales, maintains robust detection capabilities by being trained on the same comprehensive dataset. Empirical results demonstrate the effectiveness of Grounding DINO 1.5, with the Grounding DINO 1.5 Pro model attaining a 54.3 AP on the COCO detection benchmark and a 55.7 AP on the LVIS-minival zero-shot transfer benchmark, setting new records for open-set object detection. Furthermore, the Grounding DINO 1.5 Edge model, when optimized with TensorRT, achieves a speed of 75.2 FPS while attaining a zero-shot performance of 36.2 AP on the LVIS-minival benchmark, making it more suitable for edge computing scenarios. Model examples and demos with API will be released at https://github.com/IDEA-Research/Grounding-DINO-1.5-API.},
	language = {en},
	urldate = {2024-10-03},
	publisher = {arXiv},
	author = {Ren, Tianhe and Jiang, Qing and Liu, Shilong and Zeng, Zhaoyang and Liu, Wenlong and Gao, Han and Huang, Hongjie and Ma, Zhengyu and Jiang, Xiaoke and Chen, Yihao and Xiong, Yuda and Zhang, Hao and Li, Feng and Tang, Peijun and Yu, Kent and Zhang, Lei},
	month = may,
	year = {2024},
	note = {arXiv:2405.10300 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{quach_evaluating_2023,
	title = {Evaluating the {Effectiveness} of {YOLO} {Models} in {Different} {Sized} {Object} {Detection} and {Feature}-{Based} {Classification} of {Small} {Objects}},
	volume = {14},
	doi = {10.12720/jait.14.5.907-917},
	abstract = {The YOLO tool has been increasingly developed to assist in object classification. However, the problem of object classification has many difficulties, including small objects, background effects, or noise loss of information. Therefore, to evaluate the objective of classifying small, information-losing objects, the research team installed and evaluated new YOLO models such as YOLOv5, YOLOv6, and YOLOv7. In addition, the study conducted a feature-based object classification test for comparison and evaluation. The study was conducted on the self-collected data set, divided into 2 datasets: a dataset used to evaluate object classification and a dataset used to classify by features. The evaluation results show certain advantages of the YOLOv7 model on parameters such as Precision, Recall, and a mAP threshold of 50\%. The evaluation results show certain advantages of the YOLOv7 model on parameters such as Precision, Recall, and a mAP threshold of 50\%. The study results show that YOLOv7 achieves specific effects when the accuracy of object recognition is over 90\%, in which the feature-based classification also achieves an accuracy of over 70\%. This issue may need different future studies in object recognition and object feature recognition.},
	journal = {Journal of Advances in Information Technology},
	author = {Quach, Luyl Da and Nguyen, Khang and Nguyen Quynh, Anh and Tran Ngoc, Hoang},
	month = sep,
	year = {2023},
	pages = {907--917},
}

@inproceedings{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html},
	urldate = {2024-10-02},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	year = {2016},
	pages = {779--788},
}

@misc{pedoeem_yolo-lite_2018,
	title = {{YOLO}-{LITE}: {A} {Real}-{Time} {Object} {Detection} {Algorithm} {Optimized} for {Non}-{GPU} {Computers}},
	shorttitle = {{YOLO}-{LITE}},
	url = {http://arxiv.org/abs/1811.05588},
	abstract = {This paper focuses on YOLO-LITE, a real-time object detection model developed to run on portable devices such as a laptop or cellphone lacking a Graphics Processing Unit (GPU). The model was ﬁrst trained on the PASCAL VOC dataset then on the COCO dataset, achieving a mAP of 33.81\% and 12.26\% respectively. YOLO-LITE runs at about 21 FPS on a non-GPU computer and 10 FPS after implemented onto a website with only 7 layers and 482 million FLOPS. This speed is 3.8× faster than the fastest state of art model, SSD MobilenetvI. Based on the original object detection algorithm YOLOV2, YOLOLITE was designed to create a smaller, faster, and more efﬁcient model increasing the accessibility of real-time object detection to a variety of devices.},
	language = {en},
	urldate = {2024-09-29},
	publisher = {arXiv},
	author = {Pedoeem, Jonathan and Huang, Rachel},
	month = nov,
	year = {2018},
	note = {arXiv:1811.05588 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{noguchi_road_2024,
	title = {Road {Obstacle} {Detection} based on {Unknown} {Objectness} {Scores}},
	url = {http://arxiv.org/abs/2403.18207},
	doi = {10.48550/arXiv.2403.18207},
	abstract = {The detection of unknown traffic obstacles is vital to ensure safe autonomous driving. The standard object-detection methods cannot identify unknown objects that are not included under predefined categories. This is because object-detection methods are trained to assign a background label to pixels corresponding to the presence of unknown objects. To address this problem, the pixel-wise anomaly-detection approach has attracted increased research attention. Anomaly-detection techniques, such as uncertainty estimation and perceptual difference from reconstructed images, make it possible to identify pixels of unknown objects as out-of-distribution (OoD) samples. However, when applied to images with many unknowns and complex components, such as driving scenes, these methods often exhibit unstable performance. The purpose of this study is to achieve stable performance for detecting unknown objects by incorporating the object-detection fashions into the pixel-wise anomaly detection methods. To achieve this goal, we adopt a semantic-segmentation network with a sigmoid head that simultaneously provides pixel-wise anomaly scores and objectness scores. Our experimental results show that the objectness scores play an important role in improving the detection performance. Based on these results, we propose a novel anomaly score by integrating these two scores, which we term as unknown objectness score. Quantitative evaluations show that the proposed method outperforms state-of-the-art methods when applied to the publicly available datasets.},
	urldate = {2024-09-29},
	publisher = {arXiv},
	author = {Noguchi, Chihiro and Ohgushi, Toshiaki and Yamanaka, Masao},
	month = mar,
	year = {2024},
	note = {arXiv:2403.18207 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{ling_optimization_2024,
	title = {Optimization of autonomous driving image detection based on {RFAConv} and triplet attention},
	volume = {67},
	issn = {2755-2721, 2755-273X},
	url = {https://www.ewadirect.com/proceedings/ace/article/view/13908},
	doi = {10.54254/2755-2721/67/2024MA0067},
	abstract = {YOLOv8 plays a crucial role in the realm of autonomous driving, owing to its high-speed target detection, precise identification and positioning, and versatile compatibility across multiple platforms. By processing video streams or images in real-time, YOLOv8 rapidly and accurately identifies obstacles such as vehicles and pedestrians on roadways, offering essential visual data for autonomous driving systems. Moreover, YOLOv8 supports various tasks including instance segmentation, image classification, and attitude estimation, thereby providing comprehensive visual perception for autonomous driving, ultimately enhancing driving safety and efficiency. Recognizing the significance of object detection in autonomous driving scenarios and the challenges faced by existing methods, this paper proposes a holistic approach to enhance the YOLOv8 model. The study introduces two pivotal modifications: the C2f\_RFAConv module and the Triplet Attention mechanism. Firstly, the proposed modifications are elaborated upon in the methodological section. The C2f\_RFAConv module replaces the original module to enhance feature extraction efficiency, while the Triplet Attention mechanism enhances feature focus. Subsequently, the experimental procedure delineates the training and evaluation process, encompassing training the original YOLOv8, integrating modified modules, and assessing performance improvements using metrics and PR curves. The results demonstrate the efficacy of the modifications, with the improved YOLOv8 model exhibiting significant performance enhancements, including increased MAP values and improvements in PR curves. Lastly, the analysis section elucidates the results and attributes the performance improvements to the introduced modules. C2f\_RFAConv enhances feature extraction efficiency, while Triplet Attention improves feature focus for enhanced target detection.},
	language = {en},
	number = {1},
	urldate = {2024-09-29},
	journal = {Applied and Computational Engineering},
	author = {Ling, Zhipeng and Xin, Qi and Lin, Yiyu and Su, Guangze and Shui, Zuwei},
	month = jul,
	year = {2024},
	pages = {68--75},
}

@misc{liu_grounding_2024,
	title = {Grounding {DINO}: {Marrying} {DINO} with {Grounded} {Pre}-{Training} for {Open}-{Set} {Object} {Detection}},
	shorttitle = {Grounding {DINO}},
	url = {http://arxiv.org/abs/2303.05499},
	doi = {10.48550/arXiv.2303.05499},
	abstract = {In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a \$52.5\$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean \$26.1\$ AP. Code will be available at {\textbackslash}url\{https://github.com/IDEA-Research/GroundingDINO\}.},
	urldate = {2024-10-02},
	publisher = {arXiv},
	author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
	month = jul,
	year = {2024},
	note = {arXiv:2303.05499 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{jenefa_real-time_2023,
	title = {Real-{Time} {Rail} {Safety}: {A} {Deep} {Convolutional} {Neural} {Network} {Approach} for {Obstacle} {Detection} on {Tracks}},
	shorttitle = {Real-{Time} {Rail} {Safety}},
	url = {https://ieeexplore.ieee.org/document/10125284/?arnumber=10125284},
	doi = {10.1109/ICSPC57692.2023.10125284},
	abstract = {Identifying obstacles on train tracks in real-time can be challenging due to various factors such as visibility, environmental conditions, and the speed of the train. Accurate and efficient detection of obstacles is essential for ensuring the safety of passengers and avoiding derailments. The objective of this study was to develop a deep convolutional neural network (DCNN) to identify obstacles in train tracks. In recent years, the use of DCNNs has been widespread for image recognition and classification tasks, however, there has been limited research in the field of obstacle identification in train tracks. The DCNN was trained on a dataset of train track images with and without obstacles. The dataset consisted of over 1000 images with various types of obstacles, including rocks, trees, and other debris. The DCNN was implemented using the TensorFlow and Keras libraries. The DCNN was able to accurately identify obstacles in train tracks with an overall accuracy of 98\%. The model also showed high sensitivity and specificity in detecting obstacles in train tracks. This study demonstrates the effectiveness of using DCNNs for obstacle identification in train tracks. The high accuracy of the DCNN shows its potential for practical application in real-world scenarios to improve railway safety. Further research is required to test the performance of the DCNN in different environmental conditionsand to optimize its architecture for better performance.},
	urldate = {2024-09-29},
	booktitle = {2023 4th {International} {Conference} on {Signal} {Processing} and {Communication} ({ICSPC})},
	author = {Jenefa, A and Ande, Aaron and Mounikuttan, Thejas and Anuj, M.D. and Jenulin Makros, G and Rejoice, G Rachel and Shalini, T Mary},
	month = mar,
	year = {2023},
	keywords = {Deep Convolutional Neural Network (DCNN), Object Detection, Rail Safety, Rails, Railway Track, Real-time systems, Rocks, Sensitivity and specificity, Signal processing, Support vector machines, Training data},
	pages = {101--105},
}

@inproceedings{hu_novel_2019,
	title = {A {Novel} {Algorithm} for {Efficient} {Labeling} and {Its} {Application} to {On}-{Road} {Risk} {Detection}},
	url = {https://ieeexplore.ieee.org/document/8890380},
	doi = {10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00141},
	abstract = {This paper reports on the application of deep learning for on-road risk detection. In the proposed system, a USB-camera is mounted on a mobility scooter to provide video data in real-time, and a convolutional neural network (CNN) is used to detect possible risks. Each frame is classified to 11 categories, including normal, left/right attention, left/right warning, etc. A bottleneck problem in deep learning is the collection of labeled data. During the initial experiment, we collected video data containing more than 130,000 frames to train the CNN. A great number of data will be needed in the process of commercializing the system. To solve this problem, we propose a novel method that enables us to assign labels efficiently. Using this proposed method, we found experimentally that we can obtain labels of all data by labeling manually less than 10\% of all the data. In addition, the CNN obtained via transfer learning, based on the well-known AlexNet, performs very well. The average performance of several runs is about 95.83\% for testing data. Considering that 30 frames are captured in each second, this accuracy means that three consecutive mistakes are almost impossible, if we use the CNN for real-time risk detection.},
	urldate = {2024-10-02},
	booktitle = {2019 {IEEE} {Intl} {Conf} on {Dependable}, {Autonomic} and {Secure} {Computing}, {Intl} {Conf} on {Pervasive} {Intelligence} and {Computing}, {Intl} {Conf} on {Cloud} and {Big} {Data} {Computing}, {Intl} {Conf} on {Cyber} {Science} and {Technology} {Congress} ({DASC}/{PiCom}/{CBDCom}/{CyberSciTech})},
	author = {Hu, Yixin and Zhao, Qiangfu and Tomioka, Yoichi},
	month = aug,
	year = {2019},
	keywords = {Cameras, Data models, Deep learning, Labeling, Motorcycles, Real-time systems, Semisupervised learning, convolutional neural network, data labeling, deep learning, driver assistance, on-road risk detection, transfer learning},
	pages = {754--760},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2024-10-01},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
}

@article{hindarto_enhancing_2023,
	title = {Enhancing {Road} {Safety} with {Convolutional} {Neural} {Network} {Traffic} {Sign} {Classification}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by-nc/4.0},
	issn = {2541-2019, 2541-044X},
	url = {https://jurnal.polgan.ac.id/index.php/sinkron/article/view/13124},
	doi = {10.33395/sinkron.v8i4.13124},
	abstract = {Recent computer vision and deep learning breakthroughs have improved road safety by automatically classifying traffic signs. This research uses CNNs to classify traffic signs to improve road safety. Autonomous vehicles and intelligent driver assistance systems require accurate traffic sign detection and classification. Using deep learning, we created a CNN model that can recognize and classify road traffic signs. This research uses a massive dataset of labeled traffic sign photos for training and validation. These CNN algorithms evaluate images and produce realtime predictions to assist drivers and driverless cars in understanding traffic signs. Advanced driver assistance systems, navigation systems, and driverless vehicles can use this technology to give drivers more precise information, improving their decision-making and road safety. Researcher optimized CNN model design, training, and evaluation metrics during development. The model was rigorously tested and validated for robustness and classification accuracy. The research also solves realworld driving obstacles like illumination, weather, and traffic signal obstructions. This research shows deep learning-based traffic sign classification can dramatically improve road safety. This technology can prevent accidents and enhance traffic management by accurately recognizing and interpreting traffic signs. It is also a potential step toward a safer, more efficient transportation system with several automotive and intelligent transportation applications. Road safety is a global issue, and CNN-based traffic sign classification can reduce accidents and improve driving. On filter 3, Convolutional Neural Network training accuracy reached 98.9\%, while validation accuracy reached 88.23\%.},
	language = {en},
	number = {4},
	urldate = {2024-09-29},
	journal = {sinkron},
	author = {Hindarto, Djarot},
	month = nov,
	year = {2023},
	pages = {2810--2818},
}

@article{gao_obstacle_2024,
	title = {Obstacle {Detection} {Technology} for {Autonomous} {Driving} {Based} on {Deep} {Learning}},
	volume = {3},
	doi = {10.62051/c3evm786},
	abstract = {With the rapid growth of artificial intelligence (AI) technology, traditional obstacle detection equipment faces multiple challenges such as high cost, low real-time performance, non normalization, dependence on manual operation, and time-consuming and labor-intensive. To address these shortcomings, this article proposes a deep learning (DL) based obstacle detection technology for autonomous driving on the road surface. As a complex system that integrates multiple key components such as environmental perception, positioning and navigation, path planning, and motion control, one of the core technologies of autonomous vehicles is accurate perception of the surrounding environment. In practical applications, autonomous vehicles often face complex and variable road environments, which may lead to a decrease in the quality of images captured by cameras, resulting in blurry and unclear phenomena. The DL method, especially the object detection algorithm, has shown unique advantages in visual perception and recognition in autonomous driving scenes. This paper deeply studies the obstacle detection technology of automatic driving road based on DL, aiming to achieve efficient and accurate obstacle recognition, improve the safety and reliability of auto drive system, and promote the further growth of automatic driving technology.},
	journal = {Transactions on Computer Science and Intelligent Systems Research},
	author = {Gao, Chenhao},
	month = apr,
	year = {2024},
	pages = {117--122},
}

@inproceedings{farheen_object_2022,
	title = {Object {Detection} and {Navigation} {Strategy} for {Obstacle} {Avoidance} {Applied} to {Autonomous} {Wheel} {Chair} {Driving}},
	url = {https://ieeexplore.ieee.org/document/9796979},
	doi = {10.1109/IETC54973.2022.9796979},
	abstract = {The primary aim of this study is to develop machine learning or deep-learning aided procedures that enhances the capability of a commercial non-autonomous wheelchair towards autonomy. The paper addresses the computer vision work for obstacle detection applied to an autonomous wheelchair operation. The computer vision tasks including the depth image classification are accommodated in a small form factored and resource constraint computers such as Raspberry Pie and Google Coral. The tasks and strategies also include classifying the images using a pretrained model (TensorFlow lite), detecting and measure the degree of obstacle avoidance by pairing RGB image classification with depth images. The objective has been further extended to develop a simulation platform for autonomous wheelchair driving where navigation and path mapping construction algorithm evaluations are visually offered using MATLAB®.},
	urldate = {2024-09-29},
	booktitle = {2022 {Intermountain} {Engineering}, {Technology} and {Computing} ({IETC})},
	author = {Farheen, Nusrat and Jaman, Golam Gause and Schoen, Marco P.},
	month = may,
	year = {2022},
	keywords = {Computational modeling, Computer vision, Mobile robots, Navigation, Object detection, Wheelchairs, Wheels, autonomous wheelchair, convolutional neural network, depth image, mobile robot, navigation, object detection., obstacle avoidance},
	pages = {1--5},
}

@article{fang_computer_2021,
	title = {Computer vision based obstacle detection and target tracking for autonomous vehicles},
	volume = {336},
	doi = {10.1051/matecconf/202133607004},
	abstract = {Obstacle detection and target tracking are two major issues for intelligent autonomous vehicles. This paper proposes a new scheme to achieve target tracking and real-time obstacle detection of obstacles based on computer vision. ResNet-18 deep learning neural network is utilized for obstacle detection and Yolo-v3 deep learning neural network is employed for real-time target tracking. These two trained models can be deployed on an autonomous vehicle equipped with an NVIDIA Jetson Nano motherboard. The autonomous vehicle moves to avoid obstacles and follow tracked targets by camera. Adjusting the steering and movement of the autonomous vehicle according to the PID algorithm during the movement, therefore, will help the proposed vehicle achieve stable and precise tracking.},
	journal = {MATEC Web of Conferences},
	author = {Fang, Ruoyu and Cai, Cheng},
	month = feb,
	year = {2021},
	pages = {07004},
}

@inproceedings{du_fused_2017,
	title = {Fused {DNN}: {A} {Deep} {Neural} {Network} {Fusion} {Approach} to {Fast} and {Robust} {Pedestrian} {Detection}},
	shorttitle = {Fused {DNN}},
	url = {https://ieeexplore.ieee.org/abstract/document/7926694},
	doi = {10.1109/WACV.2017.111},
	abstract = {We propose a deep neural network fusion architecture for fast and robust pedestrian detection. The proposed network fusion architecture allows for parallel processing of multiple networks for speed. A single shot deep convolutional network is trained as a object detector to generate all possible pedestrian candidates of different sizes and occlusions. This network outputs a large variety of pedestrian candidates to cover the majority of ground-truth pedestrians while also introducing a large number of false positives. Next, multiple deep neural networks are used in parallel for further refinement of these pedestrian candidates. We introduce a soft-rejection based network fusion method to fuse the soft metrics from all networks together to generate the final confidence scores. Our method performs better than existing state-of-the-arts, especially when detecting small-size and occluded pedestrians. Furthermore, we propose a method for integrating pixel-wise semantic segmentation network into the network fusion architecture as a reinforcement to the pedestrian detector. The approach outperforms state-of-the-art methods on most protocols on Caltech Pedestrian dataset, with significant boosts on several protocols. It is also faster than all other methods.},
	urldate = {2024-09-29},
	booktitle = {2017 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Du, Xianzhi and El-Khamy, Mostafa and Lee, Jungwon and Davis, Larry},
	month = mar,
	year = {2017},
	keywords = {Computer architecture, Context, Detectors, Fuses, Generators, Neural networks, Semantics},
	pages = {953--961},
}

@article{dairi_obstacle_2018,
	title = {Obstacle {Detection} for {Intelligent} {Transportation} {Systems} {Using} {Deep} {Stacked} {Autoencoder} and k -{Nearest} {Neighbor} {Scheme}},
	volume = {18},
	issn = {1558-1748},
	url = {https://ieeexplore.ieee.org/document/8352801/?arnumber=8352801},
	doi = {10.1109/JSEN.2018.2831082},
	abstract = {Obstacle detection is an essential element for the development of intelligent transportation systems so that accidents can be avoided. In this paper, we propose a stereovision-based method for detecting obstacles in urban environment. The proposed method uses a deep stacked auto-encoders (DSA) model that combines the greedy learning features with the dimensionality reduction capacity and employs an unsupervised k -nearest neighbors (KNN) algorithm to accurately and reliably detect the presence of obstacles. We consider obstacle detection as an anomaly detection problem. We evaluated the proposed method by using practical data from three publicly available data sets, the Malaga stereovision urban data set, the Daimler urban segmentation data set, and the Bahnhof data set. Also, we compared the efficiency of DSA-KNN approach to the deep belief network-based clustering schemes. Results show that the DSA-KNN is suitable to visually monitor urban scenes.},
	number = {12},
	urldate = {2024-09-29},
	journal = {IEEE Sensors Journal},
	author = {Dairi, Abdelkader and Harrou, Fouzi and Sun, Ying and Senouci, Mohamed},
	month = jun,
	year = {2018},
	note = {Conference Name: IEEE Sensors Journal},
	keywords = {Clustering algorithms, Kernel, Machine learning, Machine learning algorithms, Obstacle detection, Partitioning algorithms, Roads, Sensors, autonomous vehicles, clustering algorithms, deep learning, intelligent transportation systems},
	pages = {5122--5132},
}

@inproceedings{andreev_runway_2021,
	title = {Runway {Obstacle} {Detection} for {Flight} {Vision} {Systems}},
	url = {https://ieeexplore.ieee.org/document/9396583},
	doi = {10.1109/ElConRus51938.2021.9396583},
	abstract = {Safety of aircraft operations on approach and landing presents a challenging task of air traffic management. Obstacles presence awareness and the runway condition control is extremely required in challenging weather conditions. The paper proposes the obstacles detection method for modern flight vision systems. The topic is relevant to of avionics, computer vision and image processing.},
	urldate = {2024-09-29},
	booktitle = {2021 {IEEE} {Conference} of {Russian} {Young} {Researchers} in {Electrical} and {Electronic} {Engineering} ({ElConRus})},
	author = {Andreev, Denis S.},
	month = jan,
	year = {2021},
	note = {ISSN: 2376-6565},
	keywords = {Aerospace electronics, Air traffic control, Machine vision, Meteorology, Safety, System performance, Task analysis, background subtraction, flight vision system, objects segmentation, runway},
	pages = {1596--1598},
}

@article{badrloo_image-based_2022,
	title = {Image-{Based} {Obstacle} {Detection} {Methods} for the {Safe} {Navigation} of {Unmanned} {Vehicles}: {A} {Review}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {Image-{Based} {Obstacle} {Detection} {Methods} for the {Safe} {Navigation} of {Unmanned} {Vehicles}},
	url = {https://www.mdpi.com/2072-4292/14/15/3824},
	doi = {10.3390/rs14153824},
	abstract = {Mobile robots lack a driver or a pilot and, thus, should be able to detect obstacles autonomously. This paper reviews various image-based obstacle detection techniques employed by unmanned vehicles such as Unmanned Surface Vehicles (USVs), Unmanned Aerial Vehicles (UAVs), and Micro Aerial Vehicles (MAVs). More than 110 papers from 23 high-impact computer science journals, which were published over the past 20 years, were reviewed. The techniques were divided into monocular and stereo. The former uses a single camera, while the latter makes use of images taken by two synchronised cameras. Monocular obstacle detection methods are discussed in appearance-based, motion-based, depth-based, and expansion-based categories. Monocular obstacle detection approaches have simple, fast, and straightforward computations. Thus, they are more suited for robots like MAVs and compact UAVs, which usually are small and have limited processing power. On the other hand, stereo-based methods use pair(s) of synchronised cameras to generate a real-time 3D map from the surrounding objects to locate the obstacles. Stereo-based approaches have been classified into Inverse Perspective Mapping (IPM)-based and disparity histogram-based methods. Whether aerial or terrestrial, disparity histogram-based methods suffer from common problems: computational complexity, sensitivity to illumination changes, and the need for accurate camera calibration, especially when implemented on small robots. In addition, until recently, both monocular and stereo methods relied on conventional image processing techniques and, thus, did not meet the requirements of real-time applications. Therefore, deep learning networks have been the centre of focus in recent years to develop fast and reliable obstacle detection solutions. However, we observed that despite significant progress, deep learning techniques also face difficulties in complex and unknown environments where objects of varying types and shapes are present. The review suggests that detecting narrow and small, moving obstacles and fast obstacle detection are the most challenging problem to focus on in future studies.},
	language = {en},
	number = {15},
	urldate = {2024-09-29},
	journal = {Remote Sensing},
	author = {Badrloo, Samira and Varshosaz, Masood and Pirasteh, Saied and Li, Jonathan},
	month = jan,
	year = {2022},
	note = {Number: 15
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {MAVs, UAV, deep learning methods, image-based, obstacle detection},
	pages = {3824},
}

@article{abhishek_detectron2_2021,
	title = {Detectron2 {Object} {Detection} \& {Manipulating} {Images} using {Cartoonization}},
	volume = {10},
	abstract = {In today's world, there is a rapid increase in the autonomous vehicle. There are various levels of autonomous vehicles depending upon the degree of autonomy-for the lower degree of autonomy driver has more power and functionality for managing, on coming to the fully automated vehicle like Tesla are expected to have full control over the functions. These advances cooperate to plan the vehicle's position and its nearness to everything around it. Because of this, there is popularity for these vehicles, since they give a great deal of advantages to individuals utilizing them. We use the Facebook AI Research software system that implements object detection algorithms, Caffe2 deep learning framework for advanced object detection by offering speedy training. We have also manipulated images to derive insights addressing the issues companies face when making the step from research to production. We have implemented detectron2 object detection for faster detection of objects. There is labeling of the object \& we used manipulation of images using cartoonization.},
	language = {en},
	number = {08},
	journal = {International Journal of Engineering Research},
	author = {Abhishek, Allena Venkata Sai and Kotni, Sonali},
	month = aug,
	year = {2021},
}
